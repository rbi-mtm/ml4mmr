{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdcdaf67-646e-4cf1-8ddf-20aa788e5cde",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Using Large Language Models (LLMs)\n",
    "\n",
    "This tutorial will give us an introduction to LLMs: How to use them effectively, what can we do with them, and we will explore a bit how everything works in the background!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18019c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"insert-key-here\"\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c255499",
   "metadata": {},
   "source": [
    "## 1. Using an LLM: The very basics\n",
    "\n",
    "Current LLMs are big models, so big that typically you need specialized and expensive hardware to run them (although this is [changing very rapidly](https://medium.com/@gabrielrodewald/running-models-with-ollama-step-by-step-60b6f6125807)!!).\n",
    "Also, the state-of-the-art models are not openly available, and companies provide them over an Application Programming Interface (API) so that we can have access to these amazing models without having to worry about all the [engineering details](https://github.com/mlabonne/llm-course?tab=readme-ov-file#1-running-llms) of loading and running inference with one of these huge models.\n",
    "\n",
    "#### Here we will see how to communicate with one of these APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a simple request to OpenAI directly\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "url = \"https://api.openai.com/v1/responses\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": f\"Bearer {OPENAI_API_KEY}\"\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"gpt-4o-mini\",\n",
    "    \"input\": \"Say hi to all the students in Zadar!\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data)\n",
    "print(json.dumps(response.json(), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed6a879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The response itself can be extracted from the response object\n",
    "response_json = response.json()\n",
    "answer = (\n",
    "    response_json\n",
    "    .get(\"output\", [{}])[0]\n",
    "    .get('content', [{}])[0]\n",
    "    .get(\"text\", {})\n",
    ")\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f8c13",
   "metadata": {},
   "source": [
    "## 2. LangChain\n",
    "\n",
    "#### Langchain is a framework for developing applications powered by language models.\n",
    "\n",
    "\n",
    "It aims to:\n",
    "\n",
    "- Be data-aware: connect a language model to other sources of data\n",
    "\n",
    "- Be agentic: allow a language model to interact with its environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a18dc41-93a0-4c98-a9f7-8c558480af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from utils import *\n",
    "from langchain import (\n",
    "    PromptTemplate,\n",
    "    chains\n",
    ")\n",
    "from langchain.chat_models import init_chat_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c269ceda-f18f-45ae-86e2-8c0e2460f680",
   "metadata": {},
   "source": [
    "### Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf66704-e258-42d5-b30d-da5d2334313a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's now initialize a language model.\n",
    "llm = init_chat_model(\n",
    "    \"gpt-4o-mini\",\n",
    "    model_provider=\"openai\"\n",
    ")\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "ai_message = llm.invoke([HumanMessage(\"What functional groups are present in this molecule? c1(Br)ccc(CC(=O)Cl)cc1.\")])\n",
    "ai_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8355f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9c30e2-2aee-45e0-bb21-a56be802a636",
   "metadata": {},
   "source": [
    "### Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324d771e-4c9b-4c5e-8a8c-a690d0275628",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, define a prompt template\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables = [\"smiles\"],\n",
    "    template = (\n",
    "        \"What functional groups are present in this molecule? {smiles}. \"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44c57d-2b98-4f47-9744-74cdc33a8048",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles = \"c1(Br)ccc(CC(=O)Cl)cc1\"\n",
    "\n",
    "cdk(smiles)\n",
    "prompt_template.format(smiles=smiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68546bd4-90aa-4860-90cc-bcddebe327ea",
   "metadata": {},
   "source": [
    "### Putting it together: Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce53deb9-5043-4788-ad34-beaa4f05eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "fg_chain = chains.LLMChain(\n",
    "    prompt = prompt_template,\n",
    "    llm = llm\n",
    ")\n",
    "\n",
    "cdk(smiles)\n",
    "fg_chain.invoke(smiles).get('text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c5f449-b405-4294-83d8-98f36f0f59f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_2 = \"c1ccc(-c2cncc(C3CCCCC3)c2)cc1\"\n",
    "fg_chain.invoke(smiles_2).get('text')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea508055-b999-447d-ad61-dca1380e4279",
   "metadata": {},
   "source": [
    "### Improving prompting: Formatting, in-context learning, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290abaa2-b158-459a-b418-63dc1331c1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_2 = PromptTemplate(\n",
    "    input_variables = [\"smiles\"],\n",
    "    template = (\n",
    "        \"You are an expert chemist and your task is to identify the functional groups of the given molecules.\"\\\n",
    "        \"You should give the name and the SMILES of each functional group. Begin!\"\\\n",
    "        \"Input: Brc1cncc(C2CCCCC2)c1\"\\\n",
    "        \"Output: 1. Halogen (Br)\\n 2. Pyridine (c1cnccc1)\\n 3. Cyclohexane (C2CCCCC2)\"\\\n",
    "        \"Input: c1ccccc1CC(=O)Cl\"\\\n",
    "        \"Output: 1. Phenyl (c1ccccc1)\\n 2. Carbonyl (C=O)\\n 3. Acyl halide (C(=O)Cl)\\n 4. Halogen (Cl)\"\\\n",
    "        \"Input: {smiles}\"\\\n",
    "        \"Output:\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fg_chain_new = chains.LLMChain(\n",
    "    prompt = prompt_template_2,\n",
    "    llm = llm\n",
    ")\n",
    "def fcs(smiles):\n",
    "    cdk(smiles)\n",
    "    print(fg_chain_new.invoke(smiles).get('text'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46060844-13d6-4471-81bd-17334a5a3222",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles_list = [\n",
    "    smiles,\n",
    "    smiles_2,\n",
    "    \"C/C=C/C(=O)I\",\n",
    "    \"C/C=C/C(=O)S\"\n",
    "]\n",
    "\n",
    "for s in smiles_list:\n",
    "    fcs(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de954cb",
   "metadata": {},
   "source": [
    "## 3. Applications: Data Extraction\n",
    "\n",
    "LLMs are good with language. What if they could help us turn unstructured data (like text in papers) into structured data (like a tabular database or a knowledge graph!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709e1f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a data model in Pydantic\n",
    "class Synthesis(BaseModel):\n",
    "    \"\"\"Data Model for an organic synthesis.\"\"\"\n",
    "\n",
    "    product: str = Field(description=\"The main product of the reaction\")\n",
    "    reactants: List[str] = Field(description=\"The reactants used in the synthesis\")\n",
    "    temperature: Optional[int] = Field(description=\"Temperature in degrees Celsius\")\n",
    "\n",
    "\n",
    "structured_llm = llm.with_structured_output(Synthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6aaaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Procedure taken from https://www.orgsyn.org/demo.aspx?prep=v101p0488\n",
    "text = \"\"\"(1S, 3R)-3-Methylcyclohexan-1-ol (2). A single-necked (24/40 joint) 500 mL round-bottom flask was equipped with a Teflon-coated magnetic stir bar (2.5 x 0.5 cm, pill-shaped). To the flask was added (-)-isopulegol (1) (8.5 mL, 7.7 g, 50 mmol, 1.0 equiv) (Notes 2 and 3) via syringe. Then, MeOH (200 mL, 0.25 M) (Note 4) is added at 23 ℃ to the flask from a graduated cylinder. The mixture was stirred (500 rpm) briefly at room temperature until a homogenous mixture is achieved. The flask was then placed in a saturated dry ice/Acetone bath (400 mL) (Notes 5) (1000 mL dewar) and cooled to -78 ℃ while open to the air. Ozone (Notes 6 and 7) was bubbled through the clear colorless solution (Figure 1A) for 50-100 minutes (Note 8), until complete consumption of the starting material had occurred (Figure 1B). Complete consumption can be indicated by a faint blue color (Note 9). The solution was then sparged with argon (tank pressure 11 psi) through a 16 G needle submerged in the reaction for 20 minutes to expel the excess ozone (Figure 1C).\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88099b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis = structured_llm.invoke(f\"Extract the synthesis information from the following text: {text}\")\n",
    "synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff73de10",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis.product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919c79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis.temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0116bd9",
   "metadata": {},
   "source": [
    "## 4. Applications: Agents\n",
    "\n",
    "<img src=\"agents.webp\" width=400 display=\"block\">\n",
    "\n",
    "An agent is \"something that produces or is capable of producing an effect : an active or efficient cause\".\n",
    "\n",
    "In the context of LLMs, let's consider an agent as **an entity that ---given a goal--- will plan, decide and act on its environment in order to reach its goal**.\n",
    "\n",
    "We'll be using LangChain to build agents in this tutorial, but there's plenty of frameworks that have appeared over the years.\n",
    "\n",
    "\n",
    "### Agents:\n",
    "\n",
    "We would like to integrate other things:\n",
    "- User input\n",
    "- External knowledge sources\n",
    "- External tools\n",
    "- Memory storage\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6269958-1e93-4e4c-a434-bebb8c2da3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a tool!\n",
    "from langchain.utilities import WikipediaAPIWrapper\n",
    "\n",
    "wikipedia = WikipediaAPIWrapper()\n",
    "wikipedia.run(\"Atorvastatin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183bfb50-6979-49d1-853b-2b09fa46c555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "# Define a toolset \n",
    "toolset = [\n",
    "    Tool(\n",
    "        name=\"wikipedia search\",\n",
    "        func=wikipedia.run,\n",
    "        description=\"Useful to get accurate information from wikipedia.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Define an agent\n",
    "jamesbond = initialize_agent(\n",
    "    toolset,\n",
    "    llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60bb0e2-a077-4f25-ac61-ce845280f284",
   "metadata": {},
   "outputs": [],
   "source": [
    "jamesbond.invoke(f\"What are the functional groups of {smiles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9aa90c-2967-4849-bfd6-d7c4a59fbf3c",
   "metadata": {},
   "source": [
    "# Probably not the most useful tool for this task..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52793ce9-7400-461b-a81e-51588704bb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import FuncGroups\n",
    "\n",
    "toolset += [FuncGroups()]\n",
    "\n",
    "# Define an agent\n",
    "jamesbond = initialize_agent(\n",
    "    toolset,\n",
    "    llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77eb1e82-d53a-4de1-b133-04b67b027edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "jamesbond.invoke(f\"What are the functional groups of {smiles}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca3259-643e-4d59-be59-2765e2c56dc4",
   "metadata": {},
   "source": [
    "# What's happening behind the courtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3553c68-65a6-45ba-8e60-8fbf3570c46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = jamesbond.agent.create_prompt(toolset)\n",
    "james_prompt = jamesbond.agent.llm_chain.prompt\n",
    "\n",
    "print(james_prompt.input_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e254c0-e76a-4084-9c76-b3875dd8527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(james_prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4a455e-93a1-4d3e-ad33-6145c05a17d8",
   "metadata": {},
   "source": [
    "# A more useful agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027bd269-25fe-4155-9bd8-692e9206e9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Query2SMILES\n",
    "\n",
    "toolset += [Query2SMILES()]\n",
    "\n",
    "llm = init_chat_model(\n",
    "    \"gpt-4o\",\n",
    "    model_provider=\"openai\"\n",
    ")\n",
    "\n",
    "# Define an agent\n",
    "jamesbond = initialize_agent(\n",
    "    toolset,\n",
    "    llm,\n",
    "    agent=\"zero-shot-react-description\",\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "final_answer = jamesbond.run(\n",
    "    \"Find the functional groups of cafeine, p-bromobenzaldehyde, and cyclosarin, \"\n",
    "    \"then find what they have in common, and find some information on about it in wikipedia.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7282a074-dccd-4f9d-9f3f-1c7027bd61c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
