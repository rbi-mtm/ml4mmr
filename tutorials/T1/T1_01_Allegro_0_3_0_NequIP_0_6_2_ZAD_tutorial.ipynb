{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 718
        },
        "id": "o2P3IyYHZ-mD",
        "outputId": "2d68ca8e-4576-45ab-8e7f-5d28131d17a0",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<!-- Banner -->\n",
              "<div style=\"background-color:#DCE9F8; border-radius:10px; padding:20px; display:flex; align-items:center; justify-content:space-between; margin-bottom:20px;\">\n",
              "\n",
              "  <!-- Centered Title -->\n",
              "  <div style=\"flex:1; text-align:center;\">\n",
              "    <h1 style=\"margin:0; font-size:5rem; color:#1E4D9D;\">NequIP/Allegro 0.6.2/0.3.0 Zadar Tutorial</h1>\n",
              "  </div>\n",
              "\n",
              "  <!-- Right-aligned Logo -->\n",
              "  <div style=\"flex:1; text-align:center;\">\n",
              "    <img src=\"https://github.com/mir-group/nequip/blob/main/logo.png?raw=true\" style=\"width:300px;\">\n",
              "  </div>\n",
              "</div>\n",
              "\n",
              "<!-- Tutorial Introduction -->\n",
              "<div style=\"background-color:#ffffff; border-left:0px solid #3C82E3; border-radius:10px; padding:0px; font-size:1.1rem; color:#1E4D9D; margin-bottom:20px;\">\n",
              "  <h2 style=\"margin-top:0; font-size:2rem; color:#3C82E3;\">Introduction</h2>\n",
              "  <p>This is a tutorial for <b><code>NequIP</code></b>, an architecture for building highly accurate and scalable Machine Learning Interatomic Potentials (MLIPs) and deploy them in production simulations. The ideas are described in <a href=\"https://www.nature.com/articles/s41467-022-29939-5\" target=\"_blank\" style=\"color:#3C82E3; text-decoration:none;\">E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials</a>. <b><code>NequIP</code></b> is available as an open-source package <a href=\"https://github.com/mir-group/allegro\" target=\"_blank\" style=\"color:#3C82E3; text-decoration:none;\"> HERE</a>. This tutorial serves as a simple introduction to the <b><code>NequIP</code></b> code. </p>\n",
              "</div>\n",
              "\n",
              "<!-- Contents Section -->\n",
              "<div style=\"background-color:#ffffff; border-left:0px solid #3C82E3; border-radius:10px; padding:0px; margin-bottom:20px;\">\n",
              "  <h2 style=\"margin-top:0; font-size:2rem; color:#3C82E3;\">Contents</h2>\n",
              "  <p style=\"font-size:1.1rem; color:#1E4D9D;\">\n",
              "    This tutorial will walk you through:\n",
              "  </p>\n",
              "  <ul style=\"list-style:disc; padding-left:20px; font-size:1.1rem; color:#1E4D9D;\">\n",
              "    <li style=\"margin-bottom:10px;\">\n",
              "        <b>Train</b>: Train a neural network potential using a simple dataset.\n",
              "    </li>\n",
              "    <li style=\"margin-bottom:10px;\">\n",
              "        <b>Deploy</b>: Convert the Python-based model into a stand-alone potential file optimized for fast execution.\n",
              "    </li>\n",
              "    <li style=\"margin-bottom:10px;\">\n",
              "        <b>Run</b>: Use the trained model to perform tasks such as MD in <b><code>LAMMPS</code></b>.\n",
              "    </li>\n",
              "    <!-- <li style=\"margin-bottom:10px;\">\n",
              "        <b>(Optional) Extend the model with custom code</b>\n",
              "    </li> -->\n",
              "</ul>\n",
              "\n",
              "  <p style=\"font-size:1.1rem; color:#1E4D9D;\">\n",
              "    Everything will happen in this Colab. We're ready to get started!\n",
              "  </p>\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Introduction\n",
        "%%html\n",
        "<!-- Banner -->\n",
        "<div style=\"background-color:#DCE9F8; border-radius:10px; padding:20px; display:flex; align-items:center; justify-content:space-between; margin-bottom:20px;\">\n",
        "\n",
        "  <!-- Centered Title -->\n",
        "  <div style=\"flex:1; text-align:center;\">\n",
        "    <h1 style=\"margin:0; font-size:5rem; color:#1E4D9D;\">NequIP/Allegro 0.6.2/0.3.0 Zadar Tutorial</h1>\n",
        "  </div>\n",
        "\n",
        "  <!-- Right-aligned Logo -->\n",
        "  <div style=\"flex:1; text-align:center;\">\n",
        "    <img src=\"https://github.com/mir-group/nequip/blob/main/logo.png?raw=true\" style=\"width:300px;\">\n",
        "  </div>\n",
        "</div>\n",
        "\n",
        "<!-- Tutorial Introduction -->\n",
        "<div style=\"background-color:#ffffff; border-left:0px solid #3C82E3; border-radius:10px; padding:0px; font-size:1.1rem; color:#1E4D9D; margin-bottom:20px;\">\n",
        "  <h2 style=\"margin-top:0; font-size:2rem; color:#3C82E3;\">Introduction</h2>\n",
        "  <p>This is a tutorial for <b><code>NequIP</code></b>, an architecture for building highly accurate and scalable Machine Learning Interatomic Potentials (MLIPs) and deploy them in production simulations. The ideas are described in <a href=\"https://www.nature.com/articles/s41467-022-29939-5\" target=\"_blank\" style=\"color:#3C82E3; text-decoration:none;\">E(3)-equivariant graph neural networks for data-efficient and accurate interatomic potentials</a>. <b><code>NequIP</code></b> is available as an open-source package <a href=\"https://github.com/mir-group/allegro\" target=\"_blank\" style=\"color:#3C82E3; text-decoration:none;\"> HERE</a>. This tutorial serves as a simple introduction to the <b><code>NequIP</code></b> code. </p>\n",
        "</div>\n",
        "\n",
        "<!-- Contents Section -->\n",
        "<div style=\"background-color:#ffffff; border-left:0px solid #3C82E3; border-radius:10px; padding:0px; margin-bottom:20px;\">\n",
        "  <h2 style=\"margin-top:0; font-size:2rem; color:#3C82E3;\">Contents</h2>\n",
        "  <p style=\"font-size:1.1rem; color:#1E4D9D;\">\n",
        "    This tutorial will walk you through:\n",
        "  </p>\n",
        "  <ul style=\"list-style:disc; padding-left:20px; font-size:1.1rem; color:#1E4D9D;\">\n",
        "    <li style=\"margin-bottom:10px;\">\n",
        "        <b>Train</b>: Train a neural network potential using a simple dataset.\n",
        "    </li>\n",
        "    <li style=\"margin-bottom:10px;\">\n",
        "        <b>Deploy</b>: Convert the Python-based model into a stand-alone potential file optimized for fast execution.\n",
        "    </li>\n",
        "    <li style=\"margin-bottom:10px;\">\n",
        "        <b>Run</b>: Use the trained model to perform tasks such as MD in <b><code>LAMMPS</code></b>.\n",
        "    </li>\n",
        "    <!-- <li style=\"margin-bottom:10px;\">\n",
        "        <b>(Optional) Extend the model with custom code</b>\n",
        "    </li> -->\n",
        "</ul>\n",
        "\n",
        "  <p style=\"font-size:1.1rem; color:#1E4D9D;\">\n",
        "    Everything will happen in this Colab. We're ready to get started!\n",
        "  </p>\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Setup and Installation\n",
        "%%html\n",
        "<div style=\"\n",
        "    background-color:#FFECEC;\n",
        "    border-left: 5px solid #D9534F;\n",
        "    border-radius: 5px;\n",
        "    padding: 15px;\n",
        "    font-size: 1.2rem;\n",
        "    color:#D9534F;\n",
        "    margin-bottom: 20px;\">\n",
        "\n",
        "  <p style=\"margin: 0; font-weight: bold;\">\n",
        "    <span style=\"font-size: 1.4rem;\"> ⚠ Confirm Device is <b>GPU</b> ⚠ </span>\n",
        "  </p>\n",
        "  <p style=\"margin-top: 10px; font-size: 1.1rem; color:#333;\">\n",
        "    Before you get started, make sure that in your menu bar: <br>\n",
        "    <b>Runtime</b> → <b>Change runtime type</b> is set to <b>GPU</b>.\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "<div style=\"background-color:#ffffff; border-left: 0px solid #3C82E3; border-radius: 5px; padding: 0px; margin-bottom: 20px; font-size: 1.1rem; color:#333;\">\n",
        "\n",
        "  <!-- Title -->\n",
        "  <h2 style=\"margin-top: 0; font-size: 2rem; color: #3C82E3;\">⚙️ Setup and Installations</h2>\n",
        "\n",
        "  <!-- Introduction -->\n",
        "  <p style=\"margin-bottom: 10px;\">\n",
        "    The following tools will be installed throughout the tutorial. Longest runtime being LAMMPS at ~8min\n",
        "  </p>\n",
        "\n",
        "  <!-- List of Tools -->\n",
        "  <ul style=\"list-style: disc; padding-left: 10px;\">\n",
        "    <li style=\"margin-bottom: 5px;\"><b>NequIP</b></li>\n",
        "    <li style=\"margin-bottom: 5px;\"><b>pair-nequip</b></li>\n",
        "    <li style=\"margin-bottom: 5px;\"><b>LAMMPS</b></li>\n",
        "  </ul>\n",
        "\n",
        "  <!-- Clearing env -->\n",
        "  <p style=\"margin-bottom: 10px;\">\n",
        "    Let's start by clearing the environment (mostly testing): Runtime > Disconnect and delete runtime > Reconnect\n",
        "  </p>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "cellView": "form",
        "id": "yTuIU5nxrh9v",
        "outputId": "8afa5f21-2fe9-4ac3-b92c-0f9371445260"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"\n",
              "    background-color:#FFECEC;\n",
              "    border-left: 5px solid #D9534F;\n",
              "    border-radius: 5px;\n",
              "    padding: 15px;\n",
              "    font-size: 1.2rem;\n",
              "    color:#D9534F;\n",
              "    margin-bottom: 20px;\">\n",
              "\n",
              "  <p style=\"margin: 0; font-weight: bold;\">\n",
              "    <span style=\"font-size: 1.4rem;\"> ⚠ Confirm Device is <b>GPU</b> ⚠ </span>\n",
              "  </p>\n",
              "  <p style=\"margin-top: 10px; font-size: 1.1rem; color:#333;\">\n",
              "    Before you get started, make sure that in your menu bar: <br>\n",
              "    <b>Runtime</b> → <b>Change runtime type</b> is set to <b>GPU</b>.\n",
              "  </p>\n",
              "</div>\n",
              "\n",
              "<div style=\"background-color:#ffffff; border-left: 0px solid #3C82E3; border-radius: 5px; padding: 0px; margin-bottom: 20px; font-size: 1.1rem; color:#333;\">\n",
              "\n",
              "  <!-- Title -->\n",
              "  <h2 style=\"margin-top: 0; font-size: 2rem; color: #3C82E3;\">⚙️ Setup and Installations</h2>\n",
              "\n",
              "  <!-- Introduction -->\n",
              "  <p style=\"margin-bottom: 10px;\">\n",
              "    The following tools will be installed throughout the tutorial. Longest runtime being LAMMPS at ~8min\n",
              "  </p>\n",
              "\n",
              "  <!-- List of Tools -->\n",
              "  <ul style=\"list-style: disc; padding-left: 10px;\">\n",
              "    <li style=\"margin-bottom: 5px;\"><b>NequIP</b></li>\n",
              "    <li style=\"margin-bottom: 5px;\"><b>pair-nequip</b></li>\n",
              "    <li style=\"margin-bottom: 5px;\"><b>LAMMPS</b></li>\n",
              "  </ul>\n",
              "\n",
              "  <!-- Clearing env -->\n",
              "  <p style=\"margin-bottom: 10px;\">\n",
              "    Let's start by clearing the environment (mostly testing): Runtime > Disconnect and delete runtime > Reconnect\n",
              "  </p>\n",
              "\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "## install WandB for visualization, just in case\n",
        "!pip install wandb\n",
        "\n",
        "## set anonymous WandB\n",
        "import os\n",
        "os.environ[\"WANDB_ANONYMOUS\"] = \"must\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "3gJEotU5weYX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## install Allegro 0.3.0 (automatically downloads NequIP dependency 0.6.2)\n",
        "## runtime ~3 min\n",
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "# important, torch 2.6 has a different default weights_only (from False to True)\n",
        "# that breaks loading extxyz later on\n",
        "!pip install torch==2.5.1\n",
        "\n",
        "!git clone https://github.com/mir-group/allegro --branch v0.3.0\n",
        "!pip install allegro/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "collapsed": true,
        "id": "jXWrcRK-wvYb",
        "outputId": "62a53a34-7ddb-4856-c79e-b856caafcbdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.5.1\n",
            "  Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl.metadata (28 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.1.0 (from torch==2.5.1)\n",
            "  Downloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1) (3.0.2)\n",
            "Downloading torch-2.5.1-cp311-cp311-manylinux1_x86_64.whl (906.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.5/906.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m96.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.2.0\n",
            "    Uninstalling triton-3.2.0:\n",
            "      Successfully uninstalled triton-3.2.0\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torch-2.5.1 triton-3.1.0\n",
            "Cloning into 'allegro'...\n",
            "remote: Enumerating objects: 1910, done.\u001b[K\n",
            "remote: Counting objects: 100% (1256/1256), done.\u001b[K\n",
            "remote: Compressing objects: 100% (461/461), done.\u001b[K\n",
            "remote: Total 1910 (delta 805), reused 1082 (delta 752), pack-reused 654 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1910/1910), 501.10 KiB | 1.97 MiB/s, done.\n",
            "Resolving deltas: 100% (1145/1145), done.\n",
            "Note: switching to '0cb3ec86a5850d105b95f36171460d9cadbdf401'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "Processing ./allegro\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nequip<0.7.0,>=0.6.1 (from nequip-allegro==0.3.0)\n",
            "  Downloading nequip-0.6.2-py3-none-any.whl.metadata (779 bytes)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (2.0.2)\n",
            "Collecting ase (from nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0)\n",
            "  Downloading ase-3.25.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (4.67.1)\n",
            "Collecting e3nn<0.6.0,>=0.4.4 (from nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0)\n",
            "  Downloading e3nn-0.5.6-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (6.0.2)\n",
            "Collecting torch-runstats>=0.2.0 (from nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0)\n",
            "  Downloading torch_runstats-0.2.0-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting torch-ema>=0.3.0 (from nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0)\n",
            "  Downloading torch_ema-0.3-py3-none-any.whl.metadata (415 bytes)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (1.13.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (2.5.1)\n",
            "Collecting opt_einsum_fx>=0.1.4 (from e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0)\n",
            "  Downloading opt_einsum_fx-0.1.4-py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: matplotlib>=3.3.4 in /usr/local/lib/python3.11/dist-packages (from ase->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.4->ase->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.4->ase->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.4->ase->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.4->ase->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.4->ase->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.4->ase->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.4->ase->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.4->ase->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (2.9.0.post0)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from opt_einsum_fx>=0.1.4->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (3.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (3.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.4->ase->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->e3nn<0.6.0,>=0.4.4->nequip<0.7.0,>=0.6.1->nequip-allegro==0.3.0) (3.0.2)\n",
            "Downloading nequip-0.6.2-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.4/175.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading e3nn-0.5.6-py3-none-any.whl (448 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m448.0/448.0 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_ema-0.3-py3-none-any.whl (5.5 kB)\n",
            "Downloading torch_runstats-0.2.0-py3-none-any.whl (8.1 kB)\n",
            "Downloading ase-3.25.0-py3-none-any.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opt_einsum_fx-0.1.4-py3-none-any.whl (13 kB)\n",
            "Building wheels for collected packages: nequip-allegro\n",
            "  Building wheel for nequip-allegro (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nequip-allegro: filename=nequip_allegro-0.3.0-py3-none-any.whl size=32178 sha256=b3552c7dffda3e83bebd459491d22dc3ae32b2feebcd6ebeed9eaade4fcf507b\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-wp3fcuhz/wheels/c7/69/ef/f2fd2f0e4ebce6f9dbbe0839534e9d95939e2bf5b0d12a05d2\n",
            "Successfully built nequip-allegro\n",
            "Installing collected packages: torch-runstats, ase, torch-ema, opt_einsum_fx, e3nn, nequip, nequip-allegro\n",
            "Successfully installed ase-3.25.0 e3nn-0.5.6 nequip-0.6.2 nequip-allegro-0.3.0 opt_einsum_fx-0.1.4 torch-ema-0.3 torch-runstats-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Your first model - a test\n",
        "%%html\n",
        "<div style=\"background-color:#ffffff; border-left: 0px solid #3C82E3; border-radius: 5px; padding: 0px; margin-bottom: 20px; font-size: 1.1rem; color:#333;\">\n",
        "\n",
        "  <!-- Title -->\n",
        "  <h2 style=\"margin-top: 0; font-size: 2rem; color: #3C82E3;\">🦾 Let's see if the installation went fine, let's train our first (minimal) model!</h2>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "cellView": "form",
        "id": "QY-yrv4PfBhg",
        "outputId": "66ce5c3e-ba71-4f33-82e8-8dde775851e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"background-color:#ffffff; border-left: 0px solid #3C82E3; border-radius: 5px; padding: 0px; margin-bottom: 20px; font-size: 1.1rem; color:#333;\">\n",
              "\n",
              "  <!-- Title -->\n",
              "  <h2 style=\"margin-top: 0; font-size: 2rem; color: #3C82E3;\">🦾 Let's see if the installation went fine, let's train our first (minimal) model!</h2>\n",
              "\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## minimal - runtime ~1min\n",
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "# if we are running this cell (first train) start from scratch and re-download everything\n",
        "!if [ -d \"results\" ]; then rm -rf results; fi\n",
        "!if [ -d \"benchmark_data\" ]; then rm -rf benchmark_data; fi\n",
        "!if [ -e \"test_1_out.txt\" ]; then rm -r test_1_out.txt; fi\n",
        "\n",
        "# run allegro in bg\n",
        "!nequip-train allegro/configs/minimal.yaml &> test_1_out.txt &\n",
        "# tail the output that we care about - this needs to be killed when done\n",
        "!tail -f -n 100 test_1_out.txt | grep -B 1 -E \"! Train |! Val\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "fO_G7ywhf044",
        "outputId": "053f5be1-2553-4885-c5df-0c0d702bcc6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               1    4.500    0.002        0.949        0.949         22.5         30.8\n",
            "! Validation          1    4.500    0.002        0.571        0.571         18.2         23.9\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               2    5.336    0.002        0.424        0.424         15.1         20.6\n",
            "! Validation          2    5.336    0.002        0.442        0.442         14.8         21.1\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               3    6.140    0.002        0.332        0.332           13         18.2\n",
            "! Validation          3    6.140    0.002        0.372        0.372         13.4         19.3\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               4    6.963    0.002        0.274        0.274         11.8         16.6\n",
            "! Validation          4    6.963    0.002        0.331        0.331         12.5         18.2\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               5    7.705    0.002        0.244        0.244         11.1         15.6\n",
            "! Validation          5    7.705    0.002        0.307        0.307           12         17.5\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               6    8.312    0.002        0.224        0.224         10.7           15\n",
            "! Validation          6    8.312    0.002        0.281        0.281         11.4         16.8\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               7    8.900    0.002        0.196        0.196         9.89           14\n",
            "! Validation          7    8.900    0.002        0.255        0.255         11.1           16\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               8    9.515    0.002        0.179        0.179         9.61         13.4\n",
            "! Validation          8    9.515    0.002        0.239        0.239         10.6         15.5\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               9   10.140    0.002         0.16         0.16         9.08         12.6\n",
            "! Validation          9   10.140    0.002        0.218        0.218         9.97         14.8\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train              10   10.775    0.002        0.152        0.152         8.94         12.4\n",
            "! Validation         10   10.775    0.002        0.228        0.228         10.8         15.1\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 7 test_1_out.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfrByDbGn21X",
        "outputId": "4f1f8dc3-3534-446e-c89a-5b3193fd80c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train              10   10.775    0.002        0.152        0.152         8.94         12.4\n",
            "! Validation         10   10.775    0.002        0.228        0.228         10.8         15.1\n",
            "Wall time: 10.775316051000004\n",
            "! Stop training: max epochs\n",
            "Wall time: 10.783607158999985\n",
            "Cumulative wall time: 10.783607158999985\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Let's upgrade the model\n",
        "%%html\n",
        "<div style=\"background-color:#ffffff; border-left: 0px solid #3C82E3; border-radius: 5px; padding: 0px; margin-bottom: 20px; font-size: 1.1rem; color:#333;\">\n",
        "\n",
        "  <!-- Title -->\n",
        "  <h2 style=\"margin-top: 0; font-size: 2rem; color: #3C82E3;\">Let's tweak the example file</h2>\n",
        "\n",
        "  <!-- Introduction -->\n",
        "  <p style=\"margin-bottom: 10px;\">\n",
        "    Let's take a look at the final metrics and exit condition. Can we improve?\n",
        "  </p>\n",
        "\n",
        "  <!-- List of Tools -->\n",
        "  <ul style=\"list-style: disc; padding-left: 10px;\">\n",
        "    <li style=\"margin-bottom: 5px;\">We can try to boost up the maximum iteration.</li>\n",
        "    <li style=\"margin-bottom: 5px;\">What about model angular descriptive power?</li>\n",
        "    <li style=\"margin-bottom: 5px;\">Any other suggestions?</li>\n",
        "  </ul>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "cellView": "form",
        "id": "JKUrhdujl3CB",
        "outputId": "5bffb59b-b9e0-427e-b5e4-209139ee3203"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"background-color:#ffffff; border-left: 0px solid #3C82E3; border-radius: 5px; padding: 0px; margin-bottom: 20px; font-size: 1.1rem; color:#333;\">\n",
              "\n",
              "  <!-- Title -->\n",
              "  <h2 style=\"margin-top: 0; font-size: 2rem; color: #3C82E3;\">Let's tweak the example file</h2>\n",
              "\n",
              "  <!-- Introduction -->\n",
              "  <p style=\"margin-bottom: 10px;\">\n",
              "    Let's take a look at the final metrics and exit condition. Can we improve?\n",
              "  </p>\n",
              "\n",
              "  <!-- List of Tools -->\n",
              "  <ul style=\"list-style: disc; padding-left: 10px;\">\n",
              "    <li style=\"margin-bottom: 5px;\">We can try to boost up the maximum iteration.</li>\n",
              "    <li style=\"margin-bottom: 5px;\">What about model angular descriptive power?</li>\n",
              "    <li style=\"margin-bottom: 5px;\">Any other suggestions?</li>\n",
              "  </ul>\n",
              "\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat allegro/configs/minimal.yaml | grep -e \"l_max\" -e \"max_epochs\" -e \"num_layers\" -e \"num_tensor_features\" -e \"num_bessels_per_basis\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dt5-V0E1u0oO",
        "outputId": "602b0898-0a2c-47ac-83f5-0120264600d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "l_max: 1\n",
            "num_layers: 2\n",
            "num_tensor_features: 32\n",
            "num_bessels_per_basis: 8\n",
            "max_epochs: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "variable_1_Lmax=\"2\"                             # originally 1\n",
        "variable_2_epochsmax=\"20\"                       # originally 10\n",
        "variable_3_nlayers=\"3\"                          # originally 2\n",
        "variable_4_tensor_features=\"32\"                 # originally 32\n",
        "variable_5_r_basis=\"8\"                          # originally 8\n",
        "variable_6_batch=\"1\"                            # originally 1\n",
        "\n",
        "## arch update needed\n",
        "variable_7_resnet_update=\"[1.0, 1.0, 1.0, 1.0]\" # originally size 3 vec\n",
        "\n",
        "awk '{ \\\n",
        "  if ($0 ~ /root:/) { sub(/root:.*/, \"root: results/aspirin_boosted\") }; \\\n",
        "  if ($0 ~ /run_name:/) { sub(/run_name:.*/, \"run_name: aspirin_boosted\") }; \\\n",
        "  if ($0 ~ /l_max:/) { sub(/l_max:.*/, \"l_max: '\"$variable_1_Lmax\"'\") }; \\\n",
        "  if ($0 ~ /max_epochs:/) { sub(/max_epochs:.*/, \"max_epochs: '\"$variable_2_epochsmax\"'\") }; \\\n",
        "  if ($0 ~ /num_layers:/) { sub(/num_layers:.*/, \"num_layers: '\"$variable_3_nlayers\"'\") }; \\\n",
        "  if ($0 ~ /num_tensor_features:/) { sub(/num_tensor_features:.*/, \"num_tensor_features: '\"$variable_4_tensor_features\"'\") }; \\\n",
        "  if ($0 ~ /num_bessels_per_basis:/) { sub(/num_bessels_per_basis:.*/, \"num_bessels_per_basis: '\"$variable_5_r_basis\"'\") }; \\\n",
        "  if ($0 ~ /batch_size:/) { sub(/batch_size:.*/, \"batch_size: '\"$variable_6_batch\"'\") }; \\\n",
        "  if ($0 ~ /latent_resnet_coefficients:/) { sub(/latent_resnet_coefficients:.*/, \"latent_resnet_coefficients: '\"$variable_7_resnet_update\"'\") }; \\\n",
        "  print \\\n",
        "}' allegro/configs/minimal.yaml > allegro/configs/minimal_boosted.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkSzTcritEII",
        "outputId": "463cf3f5-d6fe-4519-80f6-09883fca01f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## minimal_boosted - runtime ~3min\n",
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "!aspirin_boosted_path=\"results/aspirin_boosted\"\n",
        "!if [ -d \"$aspirin_boosted_path\" ]; then rm -rf $aspirin_boosted_path; fi\n",
        "!if [ -e \"test_2_out.txt\" ]; then rm -r test_2_out.txt; fi\n",
        "\n",
        "# run allegro in bg\n",
        "!nequip-train allegro/configs/minimal_boosted.yaml &> test_2_out.txt &\n",
        "# tail the output that we care about - this needs to be killed when done\n",
        "!tail -f -n 100 test_2_out.txt | grep -B 1 -E \"! Train |! Val\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "nMHssXtzvpWH",
        "outputId": "2c2362a8-f395-4ddf-c075-c4151837a2cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               1    4.827    0.002         1.16         1.16         24.6           34\n",
            "! Validation          1    4.827    0.002        0.574        0.574         17.4           24\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               2    6.438    0.002         0.45         0.45           15         21.2\n",
            "! Validation          2    6.438    0.002        0.427        0.427           15         20.7\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               3    7.658    0.002        0.348        0.348         13.2         18.7\n",
            "! Validation          3    7.658    0.002        0.365        0.365         13.6         19.1\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               4    8.521    0.002         0.28         0.28         11.9         16.8\n",
            "! Validation          4    8.521    0.002        0.339        0.339         12.8         18.4\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               5    9.364    0.002        0.236        0.236           11         15.4\n",
            "! Validation          5    9.364    0.002        0.279        0.279         11.7         16.7\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               6   10.253    0.002        0.202        0.202         10.3         14.2\n",
            "! Validation          6   10.253    0.002        0.241        0.241         10.8         15.5\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               7   11.121    0.002        0.175        0.175         9.49         13.2\n",
            "! Validation          7   11.121    0.002        0.217        0.217         10.6         14.8\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               8   11.986    0.002        0.149        0.149          8.9         12.2\n",
            "! Validation          8   11.986    0.002        0.228        0.228         11.1         15.1\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train               9   12.854    0.002        0.135        0.135         8.56         11.6\n",
            "! Validation          9   12.854    0.002        0.165        0.165         9.03         12.9\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train              10   13.702    0.002        0.115        0.115         7.84         10.7\n",
            "! Validation         10   13.702    0.002        0.166        0.166         9.26         12.9\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train              11   14.544    0.002       0.0996       0.0996         7.34         9.99\n",
            "! Validation         11   14.544    0.002        0.153        0.153         8.84         12.4\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train              12   15.397    0.002       0.0898       0.0898         6.98         9.49\n",
            "! Validation         12   15.397    0.002        0.137        0.137         8.34         11.7\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train              13   16.291    0.002       0.0773       0.0773         6.54          8.8\n",
            "! Validation         13   16.291    0.002        0.131        0.131         8.17         11.5\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train              14   17.153    0.002       0.0781       0.0781          6.6         8.85\n",
            "! Validation         14   17.153    0.002         0.11         0.11         7.56         10.5\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train              15   18.367    0.002        0.066        0.066         6.05         8.13\n",
            "! Validation         15   18.367    0.002        0.119        0.119         8.09         10.9\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train              16   19.610    0.002       0.0597       0.0597         5.79         7.73\n",
            "! Validation         16   19.610    0.002        0.105        0.105         7.66         10.3\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train              17   20.484    0.002       0.0562       0.0562         5.64          7.5\n",
            "! Validation         17   20.484    0.002       0.0982       0.0982         7.22         9.92\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train              18   21.335    0.002       0.0492       0.0492          5.3         7.02\n",
            "! Validation         18   21.335    0.002       0.0904       0.0904            7         9.52\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train              19   22.194    0.002       0.0474       0.0474         5.25         6.89\n",
            "! Validation         19   22.194    0.002        0.111        0.111         7.96         10.6\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f         loss        f_mae       f_rmse\n",
            "! Train              20   23.043    0.002       0.0436       0.0436         5.01         6.61\n",
            "! Validation         20   23.043    0.002       0.0831       0.0831         6.67         9.12\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Let's make ending training smarter\n",
        "%%html\n",
        "<div style=\"background-color:#ffffff; border-left: 0px solid #3C82E3; border-radius: 5px; padding: 0px; margin-bottom: 20px; font-size: 1.1rem; color:#333;\">\n",
        "\n",
        "  <!-- Title -->\n",
        "  <h2 style=\"margin-top: 0; font-size: 2rem; color: #3C82E3;\">Let's make training smarter</h2>\n",
        "\n",
        "  <!-- Introduction -->\n",
        "  <p style=\"margin-bottom: 10px;\">\n",
        "    Notice that as it stands, the training is not very smart, i.e., it goes brutally to the max number of epochs. Can we improve?\n",
        "  </p>\n",
        "\n",
        "  <!-- List of Tools -->\n",
        "  <ul style=\"list-style: disc; padding-left: 10px;\">\n",
        "    <li style=\"margin-bottom: 5px;\"><b>Important notion</b>: lowering learning rate on plateau.</li>\n",
        "    <li style=\"margin-bottom: 5px;\">Loss function is focusing only on forces.</li>\n",
        "  </ul>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "cellView": "form",
        "id": "ujfkaGqWBm-7",
        "outputId": "c7976dd2-ece9-44d3-9066-0fceed286b77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"background-color:#ffffff; border-left: 0px solid #3C82E3; border-radius: 5px; padding: 0px; margin-bottom: 20px; font-size: 1.1rem; color:#333;\">\n",
              "\n",
              "  <!-- Title -->\n",
              "  <h2 style=\"margin-top: 0; font-size: 2rem; color: #3C82E3;\">Let's make training smarter</h2>\n",
              "\n",
              "  <!-- Introduction -->\n",
              "  <p style=\"margin-bottom: 10px;\">\n",
              "    Notice that as it stands, the training is not very smart, i.e., it goes brutally to the max number of epochs. Can we improve?\n",
              "  </p>\n",
              "\n",
              "  <!-- List of Tools -->\n",
              "  <ul style=\"list-style: disc; padding-left: 10px;\">\n",
              "    <li style=\"margin-bottom: 5px;\"><b>Important notion</b>: lowering learning rate on plateau.</li>\n",
              "    <li style=\"margin-bottom: 5px;\">Loss function is focusing only on forces.</li>\n",
              "  </ul>\n",
              "\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%shell\n",
        "\n",
        "## let's update the loss function to include forces AND energies, and weight them \"appropriately\"\n",
        "loss_lines=\"\\n  forces: 1.\\n  total_energy:\\n    - 1.\\n    - PerAtomMSELoss\"\n",
        "\n",
        "## change LR on plateau\n",
        "lr_scheduler_lines=\"\\n#LR Scheduler\\nlr_scheduler_name: ReduceLROnPlateau\\nlr_scheduler_patience: 3\\nlr_scheduler_factor: 0.5\"\n",
        "\n",
        "## since we are at it, let's add some early stopping flags\n",
        "## walltime (10 mins)\n",
        "walltime_lines=\"\\n#Early stopping based on walltime\\nearly_stopping_upper_bounds:\\n  cumulative_wall: 600.\"\n",
        "## LR dropping below 1e-5\n",
        "LR_lower_bound_lines=\"\\n#Early stopping based on LR reduction\\nearly_stopping_lower_bounds:\\n  LR: 1.0e-5\"\n",
        "## no patience on no improvement in val loss\n",
        "val_loss_impatience_lines=\"\\n#Early stopping based on val loss\\nearly_stopping_patiences:\\n  validation_loss: 100\"\n",
        "\n",
        "variable_2_epochsmax=\"100\"                       # originally 10, then 20, now 100\n",
        "\n",
        "awk '{ \\\n",
        "  if ($0 ~ /root:/) { sub(/root:.*/, \"root: results/aspirin_boosted_smarter\") }; \\\n",
        "  if ($0 ~ /run_name:/) { sub(/run_name:.*/, \"run_name: aspirin_boosted_smarter\") }; \\\n",
        "  if ($0 ~ /loss_coeffs:/) { sub(/loss_coeffs:.*/, \"loss_coeffs: '\"$loss_lines\"'\") }; \\\n",
        "  if ($0 ~ /max_epochs:/) { sub(/max_epochs:.*/, \"max_epochs: '\"$variable_2_epochsmax\"'\") }; \\\n",
        "  print \\\n",
        "}' allegro/configs/minimal_boosted.yaml > allegro/configs/minimal_boosted_smarter.yaml\n",
        "\n",
        "echo -e $lr_scheduler_lines >> allegro/configs/minimal_boosted_smarter.yaml\n",
        "echo -e $walltime_lines >> allegro/configs/minimal_boosted_smarter.yaml\n",
        "echo -e $LR_lower_bound_lines >> allegro/configs/minimal_boosted_smarter.yaml\n",
        "echo -e $val_loss_impatience_lines >> allegro/configs/minimal_boosted_smarter.yaml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ugxfn2n8DCYQ",
        "outputId": "c5f75d23-051e-4d15-f031-90f2b8142862"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## minimal_boosted_smarter - runtime ~Xmin\n",
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "!aspirin_boosted_smarter_path=\"results/aspirin_boosted_smarter\"\n",
        "!if [ -d \"$aspirin_boosted_smarter_path\" ]; then rm -r $aspirin_boosted_smarter_path; fi\n",
        "!if [ -e \"test_3_out.txt\" ]; then rm -r test_3_out.txt; fi\n",
        "\n",
        "# run allegro in bg\n",
        "!nequip-train allegro/configs/minimal_boosted_smarter.yaml &> test_3_out.txt &\n",
        "# tail the output that we care about - this needs to be killed when done\n",
        "!tail -f -n 100 test_3_out.txt | grep -B 1 -E \"! Train |! Val\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "jyjL8lXmHSgC",
        "outputId": "e57a0259-f830-4cb9-df58-9176ca30a972"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train               1    4.878    0.002         1.14        0.342         1.48         24.4         33.7          284          389\n",
            "! Validation          1    4.878    0.002        0.592       0.0314        0.623         17.9         24.4          117          118\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train               2    6.119    0.002        0.491      0.00422        0.495           16         22.2         31.6         43.2\n",
            "! Validation          2    6.119    0.002        0.468      0.00317        0.471         16.1         21.7           36         37.4\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train               3    7.090    0.002        0.374      0.00253        0.377         13.7         19.4         29.9         33.5\n",
            "! Validation          3    7.090    0.002        0.396      0.00167        0.397         14.1         19.9         25.3         27.2\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train               4    8.291    0.002        0.311       0.0012        0.312         12.5         17.7         21.1         23.1\n",
            "! Validation          4    8.291    0.002         0.37      0.00214        0.372         13.5         19.2         28.6         30.7\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train               5    9.444    0.002        0.267     0.000896        0.268         11.6         16.3           18         19.9\n",
            "! Validation          5    9.444    0.002        0.307      0.00132        0.308         12.3         17.5         20.7         24.1\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train               6   10.335    0.002        0.228      0.00148        0.229         10.8         15.1         21.7         25.6\n",
            "! Validation          6   10.335    0.002        0.267     0.000571        0.268         11.4         16.4         14.4         15.9\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train               7   11.222    0.002        0.198      0.00107        0.199           10         14.1         17.1         21.7\n",
            "! Validation          7   11.222    0.002        0.242     0.000277        0.242           11         15.6         9.03         11.1\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train               8   12.112    0.002        0.166     0.000358        0.166         9.32         12.9         10.1         12.6\n",
            "! Validation          8   12.112    0.002        0.238     0.000721        0.238           11         15.4         16.2         17.9\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train               9   13.111    0.002        0.148     0.000471        0.148         8.89         12.2         11.7         14.4\n",
            "! Validation          9   13.111    0.002        0.193      0.00135        0.194         9.79         13.9         22.1         24.4\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              10   13.996    0.002        0.128     0.000389        0.128         8.26         11.3         10.3         13.1\n",
            "! Validation         10   13.996    0.002        0.191     0.000238        0.191         9.95         13.8         8.39         10.3\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              11   14.917    0.002         0.11     0.000695         0.11         7.66         10.5           14         17.5\n",
            "! Validation         11   14.917    0.002        0.171     0.000664        0.171         9.22         13.1         15.8         17.1\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              12   15.813    0.002       0.0999     0.000316          0.1         7.34           10         10.5         11.8\n",
            "! Validation         12   15.813    0.002        0.155      0.00041        0.156         8.73         12.5         13.1         13.5\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              13   16.710    0.002       0.0863     0.000104       0.0864         6.89          9.3         5.17         6.78\n",
            "! Validation         13   16.710    0.002        0.142     0.000144        0.142         8.46         11.9         5.36         7.98\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              14   17.606    0.002       0.0834     0.000283       0.0837         6.81         9.14         9.47         11.2\n",
            "! Validation         14   17.606    0.002        0.125     0.000456        0.126            8         11.2         13.7         14.2\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              15   18.497    0.002       0.0726     0.000238       0.0729         6.35         8.53         8.64         10.2\n",
            "! Validation         15   18.497    0.002        0.135     0.000269        0.136         8.63         11.7         10.6         10.9\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              16   19.558    0.002       0.0659     0.000335       0.0662         6.05         8.13         10.7         12.2\n",
            "! Validation         16   19.558    0.002        0.113     0.000187        0.113         7.85         10.7         9.05          9.1\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              17   20.845    0.002       0.0607     0.000444       0.0612         5.85          7.8         12.1           14\n",
            "! Validation         17   20.845    0.002         0.11     0.000231         0.11         7.63         10.5         9.73         10.1\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              18   21.859    0.002       0.0516     0.000472       0.0521         5.39         7.19         12.7         14.4\n",
            "! Validation         18   21.859    0.002        0.102     0.000463        0.102         7.44         10.1         12.9         14.3\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              19   22.756    0.002       0.0515     0.000151       0.0517         5.48         7.19         6.69         8.16\n",
            "! Validation         19   22.756    0.002         0.12     0.000202         0.12         8.16           11         7.01         9.46\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              20   23.640    0.002       0.0458     0.000187       0.0459         5.14         6.77         7.63         9.08\n",
            "! Validation         20   23.640    0.002       0.0902     0.000274       0.0905         7.02         9.51           10           11\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              21   24.541    0.002        0.038     0.000125       0.0381         4.71         6.17         5.59         7.43\n",
            "! Validation         21   24.541    0.002       0.0842     0.000225       0.0844         6.57         9.19          9.4         9.97\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              22   25.450    0.002       0.0367     0.000254        0.037         4.62         6.07         9.47         10.6\n",
            "! Validation         22   25.450    0.002       0.0852      0.00152       0.0867         6.78         9.24         25.3         25.9\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              23   26.382    0.002       0.0342      0.00053       0.0347         4.51         5.86         13.2         15.3\n",
            "! Validation         23   26.382    0.002       0.0837     0.000339        0.084         6.71         9.16         11.4         12.2\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              24   27.299    0.002       0.0321     0.000112       0.0322         4.28         5.67         6.32         7.03\n",
            "! Validation         24   27.299    0.002       0.0778     0.000112        0.078         6.55         8.83         6.69         7.04\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              25   28.178    0.002       0.0314     7.34e-05       0.0315          4.3         5.61         4.72          5.7\n",
            "! Validation         25   28.178    0.002       0.0725     0.000109       0.0727         6.19         8.53         6.34         6.94\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              26   29.064    0.002       0.0289     0.000104        0.029         4.16         5.38         5.86         6.79\n",
            "! Validation         26   29.064    0.002       0.0757     0.000311        0.076         6.18         8.71           11         11.7\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              27   29.962    0.002       0.0298     0.000134       0.0299          4.2         5.47         5.42         7.69\n",
            "! Validation         27   29.962    0.002       0.0724      4.1e-05       0.0724         6.18         8.52         3.63         4.26\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              28   30.898    0.002       0.0248     0.000204        0.025         3.86         4.99         7.68          9.5\n",
            "! Validation         28   30.898    0.002       0.0668     0.000191       0.0669         5.98         8.18         8.47          9.2\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              29   32.063    0.002       0.0227      0.00011       0.0228         3.68         4.77         5.37         6.97\n",
            "! Validation         29   32.063    0.002       0.0704     0.000227       0.0706         6.13          8.4         9.51           10\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              30   33.340    0.002        0.022     0.000235       0.0222         3.63         4.69          8.5         10.2\n",
            "! Validation         30   33.340    0.002        0.064     0.000372       0.0643          5.7         8.01         12.3         12.8\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              31   34.237    0.002       0.0212     0.000136       0.0214         3.54         4.61         6.24         7.75\n",
            "! Validation         31   34.237    0.002       0.0624     2.23e-05       0.0624         5.72         7.91         2.24         3.14\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              32   35.181    0.002       0.0202     0.000107       0.0203         3.48          4.5         5.83         6.87\n",
            "! Validation         32   35.181    0.002       0.0637     5.26e-05       0.0638         5.79         7.99         3.31         4.82\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              33   36.082    0.002       0.0199     0.000246       0.0202         3.47         4.47         9.02         10.4\n",
            "! Validation         33   36.082    0.002       0.0663     8.96e-05       0.0664         5.92         8.15         5.54         6.29\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              34   36.976    0.002       0.0187     7.28e-05       0.0188         3.37         4.33         5.06         5.67\n",
            "! Validation         34   36.976    0.002       0.0589     1.73e-05       0.0589         5.65         7.68         2.48         2.76\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              35   37.870    0.002       0.0174     8.87e-05       0.0175         3.25         4.17         5.29         6.26\n",
            "! Validation         35   37.870    0.002       0.0654     0.000416       0.0658         5.84         8.09         13.2         13.6\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              36   38.753    0.002       0.0177     9.02e-05       0.0178         3.28         4.21         5.04         6.32\n",
            "! Validation         36   38.753    0.002       0.0606     0.000155       0.0608         5.61         7.79         7.81         8.26\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              37   39.621    0.002        0.016     0.000209       0.0162         3.14         4.01         8.36          9.6\n",
            "! Validation         37   39.621    0.002       0.0571     0.000274       0.0574         5.49         7.56         10.7           11\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              38   40.521    0.002       0.0151     0.000131       0.0152         3.04         3.89         6.57         7.61\n",
            "! Validation         38   40.521    0.002        0.059     3.97e-05        0.059         5.49         7.69          3.1         4.19\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              39   41.442    0.002       0.0142     6.78e-05       0.0142         2.94         3.77         4.79         5.47\n",
            "! Validation         39   41.442    0.002       0.0563     2.61e-05       0.0563         5.43         7.51          3.1          3.4\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              40   42.352    0.002       0.0156     7.87e-05       0.0157         3.09         3.95         4.81          5.9\n",
            "! Validation         40   42.352    0.002       0.0597     0.000153       0.0598         5.59         7.73         7.86         8.22\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              41   43.259    0.002       0.0159     0.000146        0.016         3.11         3.99         6.86         8.02\n",
            "! Validation         41   43.259    0.002       0.0566     6.79e-05       0.0566         5.44         7.53         4.73         5.48\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              42   44.450    0.002       0.0135     7.84e-05       0.0136          2.9         3.68         5.24         5.89\n",
            "! Validation         42   44.450    0.002       0.0569     5.35e-05       0.0569         5.51         7.55         4.17         4.86\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              43   45.719    0.002       0.0122     0.000115       0.0123         2.74         3.49         5.75         7.12\n",
            "! Validation         43   45.719    0.002       0.0513     0.000247       0.0515          5.2         7.17         10.2         10.5\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              44   46.617    0.002        0.012     0.000117       0.0121          2.7         3.46         5.93         7.19\n",
            "! Validation         44   46.617    0.002       0.0538     0.000106       0.0539         5.26         7.34         6.35         6.85\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              45   47.503    0.002       0.0118     0.000138       0.0119         2.67         3.43         6.53         7.82\n",
            "! Validation         45   47.503    0.002       0.0555     1.81e-05       0.0555         5.35         7.46         1.96         2.83\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              46   48.389    0.002       0.0114     4.01e-05       0.0115         2.67         3.39         3.35         4.21\n",
            "! Validation         46   48.389    0.002       0.0521     4.28e-05       0.0521         5.32         7.22         3.98         4.35\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              47   49.272    0.002       0.0118     5.49e-05       0.0119         2.72         3.44         4.02         4.93\n",
            "! Validation         47   49.272    0.002        0.052     1.02e-05        0.052         5.23         7.22         1.78         2.12\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              48   50.154    0.001       0.0105     3.74e-05       0.0105         2.51         3.24         3.41         4.06\n",
            "! Validation         48   50.154    0.001       0.0514      5.7e-05       0.0514         5.15         7.17         4.34         5.02\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              49   51.051    0.001      0.00868     4.67e-05      0.00873         2.32         2.95         3.75         4.54\n",
            "! Validation         49   51.051    0.001         0.05     2.57e-05         0.05         5.09         7.08         2.57         3.37\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              50   51.962    0.001      0.00825     8.93e-05      0.00834         2.26         2.88         5.27         6.28\n",
            "! Validation         50   51.962    0.001       0.0505     1.63e-05       0.0505         5.05         7.11          2.4         2.68\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              51   52.883    0.001      0.00785     1.63e-05      0.00787         2.22          2.8         2.12         2.68\n",
            "! Validation         51   52.883    0.001       0.0512     7.84e-05       0.0513         5.15         7.16         5.31         5.89\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              52   53.784    0.001      0.00767     3.96e-05      0.00771         2.18         2.77         3.46         4.18\n",
            "! Validation         52   53.784    0.001       0.0507     2.48e-05       0.0508         5.11         7.13         3.09         3.31\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              53   54.679    0.001      0.00777     5.56e-05      0.00783         2.22         2.79         3.79         4.96\n",
            "! Validation         53   54.679    0.001       0.0488     4.29e-05       0.0488         5.05         6.99         3.66         4.35\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              54   55.663    0.001      0.00743     4.67e-05      0.00747         2.15         2.73         4.02         4.55\n",
            "! Validation         54   55.663    0.001       0.0481     9.76e-05       0.0482         4.93         6.95         6.06         6.57\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              55   56.875    0.001      0.00732     4.57e-05      0.00736         2.13         2.71         3.66         4.49\n",
            "! Validation         55   56.875    0.001       0.0485     6.56e-05       0.0486         4.95         6.97         4.82         5.38\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              56   58.035    0.001      0.00724     7.73e-05      0.00731         2.11         2.69         4.62         5.84\n",
            "! Validation         56   58.035    0.001       0.0479     1.28e-05       0.0479         4.91         6.93         1.85         2.38\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              57   58.925    0.001      0.00729     3.47e-05      0.00732         2.11          2.7         3.21         3.92\n",
            "! Validation         57   58.925    0.001       0.0473     4.41e-05       0.0474         4.89         6.89         3.64         4.41\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              58   59.833    0.001      0.00686     2.12e-05      0.00688         2.06         2.62         2.42         3.06\n",
            "! Validation         58   59.833    0.001       0.0491     7.79e-05       0.0491         4.94         7.01         5.31         5.87\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              59   60.758    0.001      0.00642     1.89e-05      0.00644         2.01         2.54         2.19         2.89\n",
            "! Validation         59   60.758    0.001       0.0465     2.44e-05       0.0466         4.89         6.83         2.87         3.29\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              60   61.657    0.001       0.0064     1.37e-05      0.00641         1.99         2.53         1.91         2.46\n",
            "! Validation         60   61.657    0.001       0.0468     2.25e-05       0.0468         4.83         6.84         2.85         3.15\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              61   62.533    0.001       0.0066      2.6e-05      0.00663         2.03         2.57         2.61         3.39\n",
            "! Validation         61   62.533    0.001       0.0474     1.21e-05       0.0474         4.88         6.89         1.82         2.31\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              62   63.429    0.001      0.00654     2.31e-05      0.00656         2.02         2.56         2.73          3.2\n",
            "! Validation         62   63.429    0.001       0.0466     2.25e-05       0.0466         4.82         6.83         2.61         3.15\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              63   64.307    0.001      0.00628     3.82e-05      0.00632            2         2.51         3.56         4.11\n",
            "! Validation         63   64.307    0.001       0.0474     3.68e-05       0.0474         4.94         6.89         3.66         4.03\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              64   65.193   0.0005      0.00608     3.56e-05      0.00612         1.93         2.47         2.96         3.97\n",
            "! Validation         64   65.193   0.0005       0.0456     2.76e-05       0.0456         4.79         6.76         3.15         3.49\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              65   66.103   0.0005      0.00552     1.98e-05      0.00554         1.85         2.35         2.43         2.96\n",
            "! Validation         65   66.103   0.0005       0.0465     1.43e-05       0.0465         4.86         6.82            2         2.51\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              66   66.998   0.0005      0.00575     2.26e-05      0.00577         1.89          2.4          2.8         3.16\n",
            "! Validation         66   66.998   0.0005       0.0453     1.65e-05       0.0454         4.81         6.74         2.09          2.7\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              67   68.032   0.0005      0.00549     2.82e-05      0.00552         1.84         2.35         2.81         3.53\n",
            "! Validation         67   68.032   0.0005       0.0458     4.46e-05       0.0458         4.78         6.77         3.94         4.44\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              68   69.250   0.0005      0.00546     3.85e-05      0.00549         1.85         2.34         3.37         4.12\n",
            "! Validation         68   69.250   0.0005       0.0453     1.82e-05       0.0453         4.73         6.74         2.53         2.84\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              69   70.321   0.0005      0.00537     2.45e-05      0.00539         1.82         2.32         2.66         3.29\n",
            "! Validation         69   70.321   0.0005       0.0451     3.27e-05       0.0452         4.77         6.73         3.38          3.8\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              70   71.206   0.0005      0.00524     1.59e-05      0.00526         1.81         2.29         1.95         2.65\n",
            "! Validation         70   71.206   0.0005       0.0457     2.88e-05       0.0457         4.75         6.77         3.19         3.57\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              71   72.082   0.0005      0.00526     1.88e-05      0.00528         1.82          2.3         2.29         2.89\n",
            "! Validation         71   72.082   0.0005       0.0453     4.45e-05       0.0454         4.74         6.74         3.83         4.43\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              72   72.954   0.0005       0.0053     4.56e-05      0.00535         1.81          2.3          3.5         4.49\n",
            "! Validation         72   72.954   0.0005       0.0456     9.66e-05       0.0457         4.79         6.76         6.09         6.53\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              73   73.840   0.0005      0.00532     3.88e-05      0.00536         1.82         2.31         3.09         4.14\n",
            "! Validation         73   73.840   0.0005       0.0454     1.52e-05       0.0454         4.79         6.74         2.23         2.59\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              74   74.733  0.00025      0.00488     1.14e-05      0.00489         1.74         2.21         1.78         2.24\n",
            "! Validation         74   74.733  0.00025       0.0458     2.54e-05       0.0458         4.79         6.77         3.06         3.35\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              75   75.634  0.00025      0.00484      1.1e-05      0.00486         1.73          2.2         1.81          2.2\n",
            "! Validation         75   75.634  0.00025       0.0446     2.82e-05       0.0447          4.7         6.69         3.16         3.53\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              76   76.532  0.00025      0.00483     2.01e-05      0.00486         1.74          2.2         2.33         2.98\n",
            "! Validation         76   76.532  0.00025       0.0446     4.96e-05       0.0447         4.73         6.69         4.07         4.68\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              77   77.432  0.00025      0.00488     1.25e-05      0.00489         1.73         2.21         1.92         2.35\n",
            "! Validation         77   77.432  0.00025       0.0437     1.79e-05       0.0437         4.68         6.62          2.5         2.81\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              78   78.317  0.00025      0.00477     9.59e-06      0.00478         1.73         2.19         1.65         2.06\n",
            "! Validation         78   78.317  0.00025       0.0447     1.33e-05       0.0447         4.76         6.69         1.88         2.42\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              79   79.195  0.00025      0.00482      9.2e-06      0.00483         1.73          2.2         1.72         2.02\n",
            "! Validation         79   79.195  0.00025       0.0442     2.91e-05       0.0443         4.69         6.66         3.18         3.59\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              80   80.250  0.00025      0.00472     1.12e-05      0.00473         1.71         2.18         1.59         2.23\n",
            "! Validation         80   80.250  0.00025       0.0448     1.72e-05       0.0448         4.72          6.7         2.43         2.75\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              81   81.471  0.00025      0.00472     1.21e-05      0.00473          1.7         2.17          1.9         2.31\n",
            "! Validation         81   81.471  0.00025       0.0451     2.98e-05       0.0451         4.72         6.72         3.29         3.63\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              82   82.521 0.000125      0.00455     8.21e-06      0.00456         1.68         2.14         1.58         1.91\n",
            "! Validation         82   82.521 0.000125       0.0442     1.22e-05       0.0442          4.7         6.66         1.87         2.32\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              83   83.393 0.000125      0.00452     8.89e-06      0.00453         1.68         2.13         1.59         1.98\n",
            "! Validation         83   83.393 0.000125       0.0443     2.13e-05       0.0443          4.7         6.66         2.78         3.07\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              84   84.289 0.000125      0.00451     8.36e-06      0.00452         1.67         2.13         1.58         1.92\n",
            "! Validation         84   84.289 0.000125       0.0446     1.52e-05       0.0446         4.71         6.68         2.22         2.59\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              85   85.186 0.000125      0.00452     9.76e-06      0.00453         1.68         2.13         1.72         2.08\n",
            "! Validation         85   85.186 0.000125       0.0447     1.28e-05       0.0447         4.73         6.69          1.9         2.38\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              86   86.125 6.25e-05      0.00445     8.69e-06      0.00446         1.66         2.11         1.55         1.96\n",
            "! Validation         86   86.125 6.25e-05       0.0444     1.86e-05       0.0444         4.71         6.67         2.57         2.87\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              87   87.014 6.25e-05      0.00444      1.1e-05      0.00445         1.66         2.11         1.85         2.21\n",
            "! Validation         87   87.014 6.25e-05       0.0443     1.51e-05       0.0443          4.7         6.66          2.2         2.58\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              88   87.912 6.25e-05      0.00441     9.19e-06      0.00442         1.65          2.1         1.65         2.02\n",
            "! Validation         88   87.912 6.25e-05       0.0442     1.65e-05       0.0442         4.69         6.66         2.37          2.7\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              89   88.804 6.25e-05      0.00442     1.04e-05      0.00443         1.65          2.1         1.74         2.14\n",
            "! Validation         89   88.804 6.25e-05       0.0444     1.93e-05       0.0444          4.7         6.67         2.62         2.92\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              90   89.684 3.13e-05      0.00437     8.78e-06      0.00438         1.64         2.09         1.65         1.97\n",
            "! Validation         90   89.684 3.13e-05       0.0444     1.74e-05       0.0445         4.71         6.67         2.46         2.77\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              91   90.639 3.13e-05      0.00437     8.62e-06      0.00438         1.65         2.09         1.61         1.95\n",
            "! Validation         91   90.639 3.13e-05       0.0444      1.7e-05       0.0444         4.71         6.67         2.43         2.74\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              92   91.544 3.13e-05      0.00437     8.71e-06      0.00438         1.65         2.09         1.62         1.96\n",
            "! Validation         92   91.544 3.13e-05       0.0444     1.61e-05       0.0444         4.71         6.67         2.34         2.67\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              93   92.660 3.13e-05      0.00436     8.58e-06      0.00437         1.64         2.09         1.61         1.95\n",
            "! Validation         93   92.660 3.13e-05       0.0443     1.73e-05       0.0443          4.7         6.66         2.44         2.77\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              94   93.924 1.56e-05      0.00435      8.6e-06      0.00435         1.64         2.09         1.59         1.95\n",
            "! Validation         94   93.924 1.56e-05       0.0443     1.61e-05       0.0443          4.7         6.66         2.32         2.66\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              95   94.875 1.56e-05      0.00434     8.57e-06      0.00435         1.64         2.08          1.6         1.95\n",
            "! Validation         95   94.875 1.56e-05       0.0443     1.64e-05       0.0443          4.7         6.66         2.37         2.69\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              96   95.779 1.56e-05      0.00434     8.92e-06      0.00434         1.64         2.08         1.64         1.99\n",
            "! Validation         96   95.779 1.56e-05       0.0443     1.73e-05       0.0443          4.7         6.66         2.45         2.77\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              97   96.670 1.56e-05      0.00434     8.67e-06      0.00435         1.64         2.09         1.55         1.96\n",
            "! Validation         97   96.670 1.56e-05       0.0443     1.49e-05       0.0443         4.71         6.66         2.19         2.57\n",
            "--\n",
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              98   97.541 7.81e-06      0.00433     8.46e-06      0.00434         1.64         2.08         1.57         1.93\n",
            "! Validation         98   97.541 7.81e-06       0.0443     1.59e-05       0.0443          4.7         6.66         2.31         2.65\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!tail -n 7 test_3_out.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6XQSTHZxaAs",
        "outputId": "f6306b55-4114-4df5-a871-d5c3c26a30f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae       e_rmse\n",
            "! Train              98   97.541 7.81e-06      0.00433     8.46e-06      0.00434         1.64         2.08         1.57         1.93\n",
            "! Validation         98   97.541 7.81e-06       0.0443     1.59e-05       0.0443          4.7         6.66         2.31         2.65\n",
            "Wall time: 97.54105942700005\n",
            "! Stop training: Early stopping: LR is smaller than 1e-05\n",
            "Wall time: 97.55200079999997\n",
            "Cumulative wall time: 97.55200079999997\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Let's plot\n",
        "%%html\n",
        "<div style=\"background-color:#ffffff; border-left: 0px solid #3C82E3; border-radius: 5px; padding: 0px; margin-bottom: 20px; font-size: 1.1rem; color:#333;\">\n",
        "\n",
        "  <!-- Title -->\n",
        "  <h2 style=\"margin-top: 0; font-size: 2rem; color: #3C82E3;\">Let's plot the validation of our trainings!</h2>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "ceK7HCEqxoYI",
        "outputId": "ebc8bf83-01c7-40c3-80a9-4e080ff8eda4",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"background-color:#ffffff; border-left: 0px solid #3C82E3; border-radius: 5px; padding: 0px; margin-bottom: 20px; font-size: 1.1rem; color:#333;\">\n",
              "\n",
              "  <!-- Title -->\n",
              "  <h2 style=\"margin-top: 0; font-size: 2rem; color: #3C82E3;\">Let's plot the validation of our trainings!</h2>\n",
              "\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "logfiles = [\"test_1_out.txt\", \"test_2_out.txt\", \"test_3_out.txt\"]\n",
        "what_we_want = [\"Epoch\", \"f_mae\"]\n",
        "\n",
        "def get_val_metrics(logfile_name, what_we_want_array):\n",
        "  extracted_validation_data = {col: [] for col in what_we_want_array}\n",
        "\n",
        "  with open(logfile) as f:\n",
        "    data = f.readlines()\n",
        "    data = np.array([x.split() for x in data if \"Validation\" in x.split() or \"Train\" in x.split()][1:])\n",
        "\n",
        "    for wanted in what_we_want_array:\n",
        "      index_wanted = np.where(data[0,:] == wanted)[0][0].astype(int)\n",
        "      extracted_validation_data[wanted] = data[2::3][:,index_wanted]\n",
        "\n",
        "  return extracted_validation_data\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize=(15, 5), sharex=True, sharey=True)\n",
        "for idx, logfile in enumerate(logfiles):\n",
        "\n",
        "  data = get_val_metrics(logfile, what_we_want)\n",
        "  epochs = data[\"Epoch\"].astype(int)\n",
        "  f_mae = data[\"f_mae\"].astype(float)\n",
        "  axs[idx].semilogy(epochs, f_mae)\n",
        "\n",
        "  axs[idx].set_xlabel(\"Epoch\")\n",
        "  axs[idx].set_ylabel(\"Validation f_mae\")\n",
        "  axs[idx].set_title(f\"Validation f_mae from {logfile}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "id": "l9Sgnnjr4vVA",
        "outputId": "3ad823d5-c449-4e6c-80be-0327b66d45b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ~8min\n",
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "# clone lammps\n",
        "!git clone --depth=1 https://github.com/lammps/lammps\n",
        "\n",
        "# clone pair_allegro and pair_nequip\n",
        "!git clone https://github.com/mir-group/pair_allegro.git --branch v0.6.0\n",
        "\n",
        "# patch lammps\n",
        "!cd pair_allegro && bash patch_lammps.sh ../lammps/\n",
        "\n",
        "# install mkl interface\n",
        "!pip install mkl-include\n",
        "\n",
        "# libtorch\n",
        "!wget https://download.pytorch.org/libtorch/cu102/libtorch-cxx11-abi-shared-with-deps-1.11.0%2Bcu102.zip && unzip -q libtorch-cxx11-abi-shared-with-deps-1.11.0+cu102.zip\n",
        "\n",
        "# make lammps\n",
        "!cd lammps && rm -rf build && mkdir build  && cd build && cmake ../cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_PREFIX_PATH=/content/libtorch -DMKL_INCLUDE_DIR=`python -c \"import sysconfig;from pathlib import Path;print(Path(sysconfig.get_paths()[\\\"include\\\"]).parent)\"` && make -j$(nproc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "UESYsAkfEP9o",
        "outputId": "25e8b0eb-40ee-491d-e666-34744ad17988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'lammps'...\n",
            "remote: Enumerating objects: 14417, done.\u001b[K\n",
            "remote: Counting objects: 100% (14417/14417), done.\u001b[K\n",
            "remote: Compressing objects: 100% (10366/10366), done.\u001b[K\n",
            "remote: Total 14417 (delta 4978), reused 7923 (delta 3810), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (14417/14417), 134.06 MiB | 23.43 MiB/s, done.\n",
            "Resolving deltas: 100% (4978/4978), done.\n",
            "Updating files: 100% (13752/13752), done.\n",
            "Cloning into 'pair_allegro'...\n",
            "remote: Enumerating objects: 550, done.\u001b[K\n",
            "remote: Counting objects: 100% (123/123), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 550 (delta 107), reused 96 (delta 96), pack-reused 427 (from 1)\u001b[K\n",
            "Receiving objects: 100% (550/550), 300.78 KiB | 1.55 MiB/s, done.\n",
            "Resolving deltas: 100% (311/311), done.\n",
            "Note: switching to '20538c9fd308bd0d066a716805f6f085a979c741'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by switching back to a branch.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -c with the switch command. Example:\n",
            "\n",
            "  git switch -c <new-branch-name>\n",
            "\n",
            "Or undo this operation with:\n",
            "\n",
            "  git switch -\n",
            "\n",
            "Turn off this advice by setting config variable advice.detachedHead to false\n",
            "\n",
            "Copying files...\n",
            "Updating CMakeLists.txt...\n",
            "Done!\n",
            "Collecting mkl-include\n",
            "  Downloading mkl_include-2025.1.0-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (1.3 kB)\n",
            "Downloading mkl_include-2025.1.0-py2.py3-none-manylinux_2_28_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mkl-include\n",
            "Successfully installed mkl-include-2025.1.0\n",
            "--2025-05-30 02:17:13--  https://download.pytorch.org/libtorch/cu102/libtorch-cxx11-abi-shared-with-deps-1.11.0%2Bcu102.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 18.164.154.30, 18.164.154.123, 18.164.154.17, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|18.164.154.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 813658822 (776M) [application/zip]\n",
            "Saving to: ‘libtorch-cxx11-abi-shared-with-deps-1.11.0+cu102.zip’\n",
            "\n",
            "libtorch-cxx11-abi- 100%[===================>] 775.96M  37.4MB/s    in 22s     \n",
            "\n",
            "2025-05-30 02:17:36 (35.2 MB/s) - ‘libtorch-cxx11-abi-shared-with-deps-1.11.0+cu102.zip’ saved [813658822/813658822]\n",
            "\n",
            "\u001b[0mCMake Deprecation Warning at CMakeLists.txt:21 (cmake_policy):\n",
            "  The OLD behavior for policy CMP0109 will be removed from a future version\n",
            "  of CMake.\n",
            "\n",
            "  The cmake-policies(7) manual explains that the OLD behaviors of all\n",
            "  policies are deprecated and that a policy should be set to OLD only under\n",
            "  specific short-term circumstances.  Projects should be ported to the NEW\n",
            "  behavior and not rely on setting a policy to OLD.\n",
            "\n",
            "\u001b[0m\n",
            "-- The CXX compiler identification is GNU 11.4.0\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Found Git: /usr/bin/git (found version \"2.34.1\")\n",
            "-- Appending /usr/local/cuda/lib64/stubs to CMAKE_LIBRARY_PATH: /usr/local/cuda/lib64/stubs\n",
            "-- Running check for auto-generated files from make-based build system\n",
            "-- Found MPI_CXX: /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so (found version \"3.1\")\n",
            "-- Found MPI: TRUE (found version \"3.1\")\n",
            "-- Looking for C++ include omp.h\n",
            "-- Looking for C++ include omp.h - found\n",
            "-- Found OpenMP_CXX: -fopenmp (found version \"4.5\")\n",
            "-- Found OpenMP: TRUE (found version \"4.5\") found components: CXX\n",
            "-- Found JPEG: /usr/lib/x86_64-linux-gnu/libjpeg.so (found version \"80\")\n",
            "-- Found PNG: /usr/lib/x86_64-linux-gnu/libpng.so (found version \"1.6.37\")\n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\")\n",
            "-- Found GZIP: /usr/bin/gzip\n",
            "-- Found FFMPEG: /usr/bin/ffmpeg\n",
            "-- Looking for C++ include cmath\n",
            "-- Looking for C++ include cmath - found\n",
            "-- Generating style headers...\n",
            "-- Generating package headers...\n",
            "-- Generating lmpinstalledpkgs.h...\n",
            "-- Found Python3: /usr/local/bin/python (found version \"3.11.12\") found components: Interpreter\n",
            "-- The following tools and libraries have been found and configured:\n",
            " * Git\n",
            " * MPI\n",
            " * OpenMP\n",
            " * JPEG\n",
            " * PNG\n",
            " * ZLIB\n",
            " * Python3\n",
            "\n",
            "-- <<< Build configuration >>>\n",
            "   LAMMPS Version:   20250402 8f56874-modified\n",
            "   Operating System: Linux Ubuntu\" 22.04\n",
            "   CMake Version:    3.31.6\n",
            "   Build type:       Release\n",
            "   Install path:     /root/.local\n",
            "   Generator:        Unix Makefiles using /usr/bin/gmake\n",
            "-- Enabled packages: <None>\n",
            "-- <<< Compilers and Flags: >>>\n",
            "-- C++ Compiler:     /usr/bin/c++\n",
            "      Type:          GNU\n",
            "      Version:       11.4.0\n",
            "      C++ Standard:  17\n",
            "      C++ Flags:     -O3 -DNDEBUG\n",
            "      Defines:       LAMMPS_SMALLBIG;LAMMPS_MEMALIGN=64;LAMMPS_OMP_COMPAT=4;LAMMPS_JPEG;LAMMPS_PNG;LAMMPS_GZIP;LAMMPS_FFMPEG\n",
            "-- <<< Linker flags: >>>\n",
            "-- Executable name:  lmp\n",
            "-- Static library flags:    \n",
            "-- <<< MPI flags >>>\n",
            "-- MPI_defines:      MPICH_SKIP_MPICXX;OMPI_SKIP_MPICXX;_MPICC_H\n",
            "-- MPI includes:     /usr/lib/x86_64-linux-gnu/openmpi/include;/usr/lib/x86_64-linux-gnu/openmpi/include/openmpi\n",
            "-- MPI libraries:    /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi_cxx.so;/usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so;\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD\n",
            "-- Performing Test CMAKE_HAVE_LIBC_PTHREAD - Success\n",
            "-- Found Threads: TRUE\n",
            "-- Found CUDA: /usr/local/cuda (found version \"12.5\") \n",
            "-- The CUDA compiler identification is NVIDIA 12.5.82 with host compiler GNU 11.4.0\n",
            "-- Detecting CUDA compiler ABI info\n",
            "-- Detecting CUDA compiler ABI info - done\n",
            "-- Check for working CUDA compiler: /usr/local/cuda/bin/nvcc - skipped\n",
            "-- Detecting CUDA compile features\n",
            "-- Detecting CUDA compile features - done\n",
            "-- Caffe2: CUDA detected: 12.5\n",
            "-- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n",
            "-- Caffe2: CUDA toolkit directory: /usr/local/cuda\n",
            "-- Caffe2: Header version is: 12.5\n",
            "-- Found CUDNN: /usr/lib/x86_64-linux-gnu/libcudnn.so\n",
            "-- Found cuDNN: v9.2.1  (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n",
            "-- /usr/local/cuda/lib64/libnvrtc.so shorthash is a50b0e02\n",
            "-- Autodetected CUDA architecture(s):  7.5\n",
            "-- Added CUDA NVCC flags for: -gencode;arch=compute_75,code=sm_75\n",
            "-- Found Torch: /content/libtorch/lib/libtorch.so\n",
            "-- Configuring done (10.9s)\n",
            "-- Generating done (0.2s)\n",
            "-- Build files have been written to: /content/lammps/build\n",
            "[  0%] \u001b[34m\u001b[1mGenerating includes/lammps/fmt/core.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/variable.h\u001b[0m\n",
            "[  1%] Built target fmt_core.h\n",
            "[  1%] Built target variable.h\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/fmt/format.h\u001b[0m\n",
            "[  1%] Built target fmt_format.h\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/lmppython.h\u001b[0m\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/lmptype.h\u001b[0m\n",
            "[  1%] Built target lmppython.h\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/memory.h\u001b[0m\n",
            "[  1%] Built target lmptype.h\n",
            "[  1%] Built target memory.h\n",
            "[  1%] \u001b[34m\u001b[1mGenerating includes/lammps/kspace.h\u001b[0m\n",
            "[  1%] Built target kspace.h\n",
            "[  2%] \u001b[34m\u001b[1mGenerating includes/lammps/modify.h\u001b[0m\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/lammps.h\u001b[0m\n",
            "[  3%] Built target modify.h\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/lattice.h\u001b[0m\n",
            "[  3%] Built target lammps.h\n",
            "[  3%] Built target lattice.h\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/library.h\u001b[0m\n",
            "[  3%] Built target library.h\n",
            "[  3%] \u001b[34m\u001b[1mGenerating includes/lammps/fix.h\u001b[0m\n",
            "[  3%] Built target fix.h\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/force.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/group.h\u001b[0m\n",
            "[  4%] Built target force.h\n",
            "[  4%] Built target group.h\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/platform.h\u001b[0m\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/pointers.h\u001b[0m\n",
            "[  4%] Built target platform.h\n",
            "[  4%] Built target pointers.h\n",
            "[  4%] \u001b[34m\u001b[1mGenerating includes/lammps/region.h\u001b[0m\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/timer.h\u001b[0m\n",
            "[  5%] Built target region.h\n",
            "[  5%] Built target timer.h\n",
            "[  5%] \u001b[34m\u001b[1mGenerating includes/lammps/improper.h\u001b[0m\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/input.h\u001b[0m\n",
            "[  6%] Built target improper.h\n",
            "[  6%] Built target input.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/info.h\u001b[0m\n",
            "-- Git Directory: /content/lammps/.git\n",
            "[  6%] Built target info.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/angle.h\u001b[0m\n",
            "[  6%] Built target angle.h\n",
            "-- Generating lmpgitversion.h...\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/atom.h\u001b[0m\n",
            "[  6%] Built target gitversion\n",
            "[  6%] Built target atom.h\n",
            "[  6%] \u001b[34m\u001b[1mGenerating includes/lammps/domain.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/error.h\u001b[0m\n",
            "[  7%] Built target domain.h\n",
            "[  7%] Built target error.h\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/exceptions.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/neighbor.h\u001b[0m\n",
            "[  7%] Built target exceptions.h\n",
            "[  7%] Built target neighbor.h\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/neigh_list.h\u001b[0m\n",
            "[  7%] \u001b[34m\u001b[1mGenerating includes/lammps/output.h\u001b[0m\n",
            "[  7%] Built target neigh_list.h\n",
            "[  7%] Built target output.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/pair.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/bond.h\u001b[0m\n",
            "[  8%] Built target pair.h\n",
            "[  8%] Built target bond.h\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/citeme.h\u001b[0m\n",
            "[  8%] \u001b[34m\u001b[1mGenerating includes/lammps/comm.h\u001b[0m\n",
            "[  8%] Built target citeme.h\n",
            "[  8%] Built target comm.h\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/command.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/compute.h\u001b[0m\n",
            "[  9%] Built target command.h\n",
            "[  9%] Built target compute.h\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/dihedral.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/universe.h\u001b[0m\n",
            "[  9%] Built target dihedral.h\n",
            "[  9%] Built target universe.h\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/update.h\u001b[0m\n",
            "[  9%] \u001b[34m\u001b[1mGenerating includes/lammps/utils.h\u001b[0m\n",
            "[  9%] Built target update.h\n",
            "[  9%] Built target utils.h\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle.cpp.o\u001b[0m\n",
            "[  9%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_deprecated.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_hybrid.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_write.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/angle_zero.cpp.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/arg_info.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_map.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec.cpp.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_atomic.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_body.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_charge.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_ellipsoid.cpp.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_hybrid.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_line.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_sphere.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/atom_vec_tri.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/balance.cpp.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/body.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_deprecated.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_hybrid.cpp.o\u001b[0m\n",
            "[ 14%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/bond_zero.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/change_box.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/citeme.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm.cpp.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_brick.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/comm_tiled.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_aggregate_atom.cpp.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_allegro.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angle_local.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_angmom_chunk.cpp.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_bond_local.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centro_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_centroid_stress_atom.cpp.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_chunk_spread_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cluster_atom.cpp.o\u001b[0m\n",
            "[ 19%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_cna_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_com_chunk.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_coord_atom.cpp.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_count_type.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_deprecated.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dihedral_local.cpp.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_dipole_chunk.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_displace_atom.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere.cpp.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_erotate_sphere_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_fragment_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_global_atom.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_group_group.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration.cpp.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_gyration_chunk.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_heat_flux.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_improper_local.cpp.o\u001b[0m\n",
            "[ 24%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_inertia_chunk.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_ke_atom.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd.cpp.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_msd_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_omega_chunk.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_orientorder_atom.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair.cpp.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pair_local.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pe_atom.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_pressure.cpp.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_atom.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_chunk.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_grid.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_property_local.cpp.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_rdf.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_chunk.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_reduce_region.cpp.o\u001b[0m\n",
            "[ 29%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_slice.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_stress_atom.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_chunk.cpp.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_com.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_deform.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_partial.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_profile.cpp.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_ramp.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_region.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_temp_sphere.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_torque_chunk.cpp.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vacf.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vacf_chunk.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/compute_vcm_chunk.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_atoms.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_bonds.cpp.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/create_box.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_atoms.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/delete_bonds.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/deprecated.cpp.o\u001b[0m\n",
            "[ 34%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_deprecated.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_hybrid.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_write.cpp.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dihedral_zero.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/displace_atoms.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/domain.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump.cpp.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_atom.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_cfg.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_custom.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_deprecated.cpp.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_grid.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_grid_vtk.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_image.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_local.cpp.o\u001b[0m\n",
            "[ 38%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_movie.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/dump_xyz.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/error.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/finish.cpp.o\u001b[0m\n",
            "[ 39%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_adapt.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_addforce.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_atom.cpp.o\u001b[0m\n",
            "[ 40%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_chunk.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_correlate.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_grid.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo.cpp.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_histo_weight.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_ave_time.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_aveforce.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_balance.cpp.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_bond_history.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_box_relax.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deform.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deposit.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_deprecated.cpp.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dt_reset.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_dummy.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_efield.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_enforce2d.cpp.o\u001b[0m\n",
            "[ 44%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_evaporate.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_external.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_gravity.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_group.cpp.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_halt.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_heat.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_indent.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_langevin.cpp.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_lineforce.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_minimize.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_momentum.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_move.cpp.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_neigh_history.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nh_sphere.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph.cpp.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nph_sphere.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_npt_sphere.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve.cpp.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_limit.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_noforce.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nve_sphere.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt.cpp.o\u001b[0m\n",
            "[ 51%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sllod.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_nvt_sphere.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_pair.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_planeforce.cpp.o\u001b[0m\n",
            "[ 52%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_berendsen.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_press_langevin.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_print.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_property_atom.cpp.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_read_restart.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_recenter.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_respa.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_restrain.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_setforce.cpp.o\u001b[0m\n",
            "[ 54%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_chunk.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_spring_self.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_atom.cpp.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_force.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_global.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_local.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_store_state.cpp.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_berendsen.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_temp_rescale.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_thermal_conductivity.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_vector.cpp.o\u001b[0m\n",
            "[ 57%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_viscous.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_harmonic.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj1043.cpp.o\u001b[0m\n",
            "[ 58%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj126.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_lj93.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_morse.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_reflect.cpp.o\u001b[0m\n",
            "[ 59%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_region.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fix_wall_table.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_format.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/fmtlib_os.cpp.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/force.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/grid2d.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/grid3d.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/group.cpp.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/hashlittle.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/image.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_group.cpp.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_neigh.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_store.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_time.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/imbalance_var.cpp.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_deprecated.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_hybrid.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/improper_zero.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/info.cpp.o\u001b[0m\n",
            "[ 64%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/input.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/integrate.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/irregular.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace.cpp.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/kspace_deprecated.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/label_map.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lammps.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lattice.cpp.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/library.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/lmppython.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_eigen.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_extra.cpp.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/math_special.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/memory.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_cg.cpp.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_deprecated.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_fire.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_hftn.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_linesearch.cpp.o\u001b[0m\n",
            "[ 69%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_quickmin.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/min_sd.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/minimize.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/modify.cpp.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/molecule.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_page.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/my_pool_chunk.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin.cpp.o\u001b[0m\n",
            "[ 71%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_multi.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nbin_standard.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_list.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neigh_request.cpp.o\u001b[0m\n",
            "[ 72%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/neighbor.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_bin.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_bin_ghost.cpp.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_copy.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_halffull.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_multi.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_multi_old.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_nsq.cpp.o\u001b[0m\n",
            "[ 74%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_nsq_ghost.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_respa_bin.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_respa_nsq.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip.cpp.o\u001b[0m\n",
            "[ 75%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_respa.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_skip_size_off2on_oneside.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/npair_trim.cpp.o\u001b[0m\n",
            "[ 76%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_bin.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_ghost_bin.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_multi.cpp.o\u001b[0m\n",
            "[ 77%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/nstencil_multi_old.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_all.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_partial.cpp.o\u001b[0m\n",
            "[ 78%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_angle_template.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_all.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_partial.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_bond_template.cpp.o\u001b[0m\n",
            "[ 79%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_partial.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_dihedral_template.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_all.cpp.o\u001b[0m\n",
            "[ 80%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_partial.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/ntopo_improper_template.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/output.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair.cpp.o\u001b[0m\n",
            "[ 81%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_allegro.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_born.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_buck_coul_cut.cpp.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_cut.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_debye.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_dsf.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_coul_wolf.cpp.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_deprecated.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_molecular.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_overlay.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_hybrid_scaled.cpp.o\u001b[0m\n",
            "[ 84%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_cut_coul_cut.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_lj_expand.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_morse.cpp.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_soft.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_table.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_yukawa.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zbl.cpp.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/pair_zero.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/platform.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/potential_file_reader.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/procmap.cpp.o\u001b[0m\n",
            "[ 87%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_mars.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/random_park.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rcb.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_data.cpp.o\u001b[0m\n",
            "[ 88%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_dump.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/read_restart.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_native.cpp.o\u001b[0m\n",
            "[ 89%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reader_xyz.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_block.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cone.cpp.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_cylinder.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_deprecated.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_ellipsoid.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_intersect.cpp.o\u001b[0m\n",
            "[ 91%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_plane.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_prism.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_sphere.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/region_union.cpp.o\u001b[0m\n",
            "[ 92%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/replicate.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/rerun.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_id.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_image.cpp.o\u001b[0m\n",
            "[ 93%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/reset_atoms_mol.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/respa.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/run.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/set.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/special.cpp.o\u001b[0m\n",
            "[ 94%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/table_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tabular_function.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/text_file_reader.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/thermo.cpp.o\u001b[0m\n",
            "[ 95%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/timer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/tokenizer.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/universe.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/update.cpp.o\u001b[0m\n",
            "[ 96%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/utils.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/variable.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/velocity.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/verlet.cpp.o\u001b[0m\n",
            "[ 97%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_coeff.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_data.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_dump.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lammps.dir/content/lammps/src/write_restart.cpp.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX static library liblammps.a\u001b[0m\n",
            "[ 98%] Built target lammps\n",
            "[ 98%] \u001b[32mBuilding CXX object CMakeFiles/lmp.dir/content/lammps/src/main.cpp.o\u001b[0m\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable lmp\u001b[0m\n",
            "[100%] Built target lmp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download the Si data from Gdrive\n",
        "!gdown --no-cookies 1d05GjO0hSk1WHj7YgU49EpUUYWAa54wm --output sitraj.extxyz\n",
        "!gdown --no-cookies 1FMtGNcK9TNiWFcc9fOEkQSnnFNmtvG83 --output Si_tutorial.yaml\n",
        "!mkdir Si_info\n",
        "!mv sitraj.extxyz ./Si_info/\n",
        "!mv Si_tutorial.yaml ./Si_info/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRIvvR0rIise",
        "outputId": "e893920e-d70b-4d85-e753-ff305d0a1cf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1d05GjO0hSk1WHj7YgU49EpUUYWAa54wm\n",
            "To: /content/sitraj.extxyz\n",
            "100% 785k/785k [00:00<00:00, 8.59MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1FMtGNcK9TNiWFcc9fOEkQSnnFNmtvG83\n",
            "To: /content/Si_tutorial.yaml\n",
            "100% 2.64k/2.64k [00:00<00:00, 13.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## minimal_boosted_smarter - runtime ~3min\n",
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "!rm -rf ./results/silicon-tutorial\n",
        "!nequip-train ./Si_info/Si_tutorial.yaml &> Si_tutorial_out.txt &\n",
        "# tail the output that we care about - this needs to be killed when done\n",
        "!tail -f -n 100 Si_tutorial_out.txt | grep -B 1 -E \"! Train |! Val\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "lpbURH4VLYPD",
        "outputId": "9de4d018-6137-4aaf-a826-4fc089c16f86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Train      #    Epoch      wal       LR       loss_f       loss_e         loss        f_mae       f_rmse        e_mae      e/N_mae\n",
            "! Train               1    4.686    0.002        0.148       0.0199        0.168         0.23        0.347          5.1       0.0796\n",
            "! Validation          1    4.686    0.002        0.049     5.57e-06        0.049        0.145          0.2         0.11      0.00172\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Let's lmp\n",
        "%%html\n",
        "<div style=\"background-color:#ffffff; border-left: 0px solid #3C82E3; border-radius: 5px; padding: 0px; margin-bottom: 20px; font-size: 1.1rem; color:#333;\">\n",
        "\n",
        "  <!-- Title -->\n",
        "  <h2 style=\"margin-top: 0; font-size: 2rem; color: #3C82E3;\">Let's LAMMPS!</h2>\n",
        "\n",
        "  <!-- Introduction -->\n",
        "  <p style=\"margin-bottom: 10px;\">\n",
        "    We will be doing the following three things:\n",
        "  </p>\n",
        "\n",
        "  <!-- List of Tools -->\n",
        "  <ul style=\"list-style: disc; padding-left: 10px;\">\n",
        "    <li style=\"margin-bottom: 5px;\">Deploy (compile) the learned model.</li>\n",
        "    <li style=\"margin-bottom: 5px;\">Generate files for LAMMPS</li>\n",
        "    <li style=\"margin-bottom: 5px;\">Run!</li>\n",
        "  </ul>\n",
        "\n",
        "</div>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "cellView": "form",
        "id": "TmoFoIzNBy-a",
        "outputId": "86b8fdaa-45d0-4ced-fb26-04dda730e79f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div style=\"background-color:#ffffff; border-left: 0px solid #3C82E3; border-radius: 5px; padding: 0px; margin-bottom: 20px; font-size: 1.1rem; color:#333;\">\n",
              "\n",
              "  <!-- Title -->\n",
              "  <h2 style=\"margin-top: 0; font-size: 2rem; color: #3C82E3;\">Let's LAMMPS!</h2>\n",
              "\n",
              "  <!-- Introduction -->\n",
              "  <p style=\"margin-bottom: 10px;\">\n",
              "    We will be doing the following three things:\n",
              "  </p>\n",
              "\n",
              "  <!-- List of Tools -->\n",
              "  <ul style=\"list-style: disc; padding-left: 10px;\">\n",
              "    <li style=\"margin-bottom: 5px;\">Deploy (compile) the learned model.</li>\n",
              "    <li style=\"margin-bottom: 5px;\">Generate files for LAMMPS</li>\n",
              "    <li style=\"margin-bottom: 5px;\">Run!</li>\n",
              "  </ul>\n",
              "\n",
              "</div>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# deploy\n",
        "!nequip-deploy build --train-dir results/silicon-tutorial/Si-tutorial si-deployed.pth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwJqY1ZxBcco",
        "outputId": "bb63a356-ed54-4c46-f92e-480bae5e8d68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/nequip/__init__.py:20: UserWarning: !! PyTorch version 2.5.1 found. Upstream issues in PyTorch versions 1.13.* and 2.* have been seen to cause unusual performance degredations on some CUDA systems that become worse over time; see https://github.com/mir-group/nequip/discussions/311. The best tested PyTorch version to use with CUDA devices is 1.11; while using other versions if you observe this problem, an unexpected lack of this problem, or other strange behavior, please post in the linked GitHub issue.\n",
            "  warnings.warn(\n",
            "INFO:root:Loading best_model.pth from training session...\n",
            "/usr/local/lib/python3.11/dist-packages/nequip/scripts/deploy.py:327: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(\n",
            "INFO:root:Compiled & optimized model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# info for lammps\n",
        "\n",
        "# 1: the structure\n",
        "from ase.io import read, write\n",
        "from os import system\n",
        "\n",
        "Si_str_start = read('./Si_info/sitraj.extxyz', index=0)\n",
        "system(f\"mkdir ./Si_run\")\n",
        "write('./Si_run/si.data', Si_str_start, format='lammps-data')\n",
        "\n",
        "# 2: the input file\n",
        "lammps_input = \"\"\"\n",
        "units\tmetal\n",
        "atom_style atomic\n",
        "dimension 3\n",
        "\n",
        "# set newton on for pair_allegro (off for pair_nequip)\n",
        "newton on\n",
        "boundary p p p\n",
        "read_data ./si.data\n",
        "\n",
        "# let's make it bigger\n",
        "replicate 3 3 3\n",
        "\n",
        "# allegro pair style\n",
        "pair_style\tallegro\n",
        "pair_coeff\t* * ../si-deployed.pth Si\n",
        "\n",
        "mass 1 28.0855\n",
        "\n",
        "velocity all create 300.0 1234567 loop geom\n",
        "\n",
        "neighbor 1.0 bin\n",
        "neigh_modify delay 5 every 1\n",
        "\n",
        "timestep 0.001\n",
        "thermo 10\n",
        "\n",
        "# nose-hoover thermostat, 300K\n",
        "fix  1 all nvt temp 300 300 $(100*dt)\n",
        "\n",
        "# compute rdf and average after some equilibration\n",
        "comm_modify cutoff 7.0\n",
        "compute rdfall all rdf 1000 cutoff 5.0\n",
        "fix 2 all ave/time 1 2500 5000 c_rdfall[*] file si.rdf mode vector\n",
        "\n",
        "# run 5ps\n",
        "run 5000\n",
        "\"\"\"\n",
        "with open(\"Si_run/si_rdf.in\", \"w\") as f:\n",
        "    f.write(lammps_input)"
      ],
      "metadata": {
        "id": "iwL8WqB3CQHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## runtime ~3min\n",
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "# 3: run lammps!\n",
        "!cd ./Si_run && ../lammps/build/lmp -in si_rdf.in"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "v29WCb73DZ2J",
        "outputId": "8ff9fe31-9252-4665-8eea-2ac593e133a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LAMMPS (2 Apr 2025 - Development - 8f56874-modified)\n",
            "OMP_NUM_THREADS environment is not set. Defaulting to 1 thread. (src/comm.cpp:99)\n",
            "  using 1 OpenMP thread(s) per MPI task\n",
            "Reading data file ...\n",
            "  orthogonal box = (0 0 0) to (10.862 10.862 10.862)\n",
            "  1 by 1 by 1 MPI processor grid\n",
            "  reading atoms ...\n",
            "  64 atoms\n",
            "  read_data CPU = 0.002 seconds\n",
            "Replication is creating a 3x3x3 = 27 times larger system...\n",
            "  orthogonal box = (0 0 0) to (32.586 32.586 32.586)\n",
            "  1 by 1 by 1 MPI processor grid\n",
            "  1728 atoms\n",
            "  replicate CPU = 0.001 seconds\n",
            "Allegro is using input precision f and output precision d\n",
            "Allegro: Loading model from ../si-deployed.pth\n",
            "Allegro: Freezing TorchScript model...\n",
            "Type mapping:\n",
            "Allegro type | Allegro name | LAMMPS type | LAMMPS name\n",
            "0 | Si | 1 | Si\n",
            "Neighbor list info ...\n",
            "  update: every = 1 steps, delay = 5 steps, check = yes\n",
            "  max neighbors/atom: 2000, page size: 100000\n",
            "  master list distance cutoff = 6\n",
            "  ghost atom cutoff = 7\n",
            "  binsize = 3, bins = 11 11 11\n",
            "  2 neighbor lists, perpetual/occasional/extra = 1 1 0\n",
            "  (1) pair allegro, perpetual\n",
            "      attributes: full, newton on, ghost\n",
            "      pair build: full/bin/ghost\n",
            "      stencil: full/ghost/bin/3d\n",
            "      bin: standard\n",
            "  (2) compute rdf, occasional\n",
            "      attributes: half, newton on\n",
            "      pair build: half/bin/atomonly/newton\n",
            "      stencil: half/bin/3d\n",
            "      bin: standard\n",
            "Setting up Verlet run ...\n",
            "  Unit style    : metal\n",
            "  Current step  : 0\n",
            "  Time step     : 0.001\n",
            "Per MPI rank memory allocation (min/avg/max) = 4.625 | 4.625 | 4.625 Mbytes\n",
            "   Step          Temp          E_pair         E_mol          TotEng         Press     \n",
            "         0   300           -224751.58      0             -224684.61      99647.946    \n",
            "        10   195.52862     -224728.19      0             -224684.54      100217.37    \n",
            "        20   94.055848     -224705.31      0             -224684.31      99846.426    \n",
            "        30   157.07108     -224718.92      0             -224683.86      97435.36     \n",
            "        40   178.82521     -224723.04      0             -224683.12      95999.17     \n",
            "        50   94.494214     -224703.7       0             -224682.61      96367.204    \n",
            "        60   91.293714     -224702.64      0             -224682.26      96188.836    \n",
            "        70   169.10244     -224719.39      0             -224681.64      95587.549    \n",
            "        80   154.4846      -224715.29      0             -224680.81      96654.116    \n",
            "        90   114.5828      -224705.72      0             -224680.14      98161.147    \n",
            "       100   181.78243     -224719.97      0             -224679.39      98100.519    \n",
            "       110   221.36602     -224727.73      0             -224678.32      97987.467    \n",
            "       120   155.6106      -224712.11      0             -224677.37      98796.265    \n",
            "       130   142.83326     -224708.51      0             -224676.63      98702.896    \n",
            "       140   203.55414     -224721.11      0             -224675.67      97692.563    \n",
            "       150   187.4383      -224716.41      0             -224674.57      97768.291    \n",
            "       160   141.20649     -224705.21      0             -224673.69      98209.529    \n",
            "       170   182.14766     -224713.46      0             -224672.8       97432.769    \n",
            "       180   208.77615     -224718.26      0             -224671.65      96873.842    \n",
            "       190   150.66117     -224704.26      0             -224670.63      97603.212    \n",
            "       200   144.55114     -224702.07      0             -224669.8       97820.651    \n",
            "       210   217.33571     -224717.24      0             -224668.72      97046.932    \n",
            "       220   208.73969     -224714.06      0             -224667.46      97338.186    \n",
            "       230   144.18522     -224698.66      0             -224666.47      98402.925    \n",
            "       240   181.43792     -224706.04      0             -224665.54      98105.705    \n",
            "       250   244.44091     -224718.82      0             -224664.25      97301.291    \n",
            "       260   200.04063     -224707.64      0             -224662.99      97726.975    \n",
            "       270   154.98786     -224696.61      0             -224662.01      98096.942    \n",
            "       280   206.74854     -224707.11      0             -224660.96      97263.656    \n",
            "       290   232.78308     -224711.59      0             -224659.63      97024.006    \n",
            "       300   187.68851     -224700.33      0             -224658.43      98009.985    \n",
            "       310   193.03569     -224700.45      0             -224657.36      98316.467    \n",
            "       320   244.86402     -224710.74      0             -224656.08      97775.23     \n",
            "       330   229.02949     -224705.87      0             -224654.74      98014.476    \n",
            "       340   196.78533     -224697.53      0             -224653.6       98488.787    \n",
            "       350   234.81538     -224704.83      0             -224652.41      97847.366    \n",
            "       360   251.53955     -224707.21      0             -224651.06      97218.418    \n",
            "       370   198.60051     -224694.18      0             -224649.85      97607.199    \n",
            "       380   187.49226     -224690.65      0             -224648.8       97756.948    \n",
            "       390   245.85518     -224702.43      0             -224647.54      97198.361    \n",
            "       400   261.1424      -224704.41      0             -224646.11      97305.503    \n",
            "       410   219.33769     -224693.82      0             -224644.86      98118.806    \n",
            "       420   229.84752     -224695.01      0             -224643.7       98086.14     \n",
            "       430   271.05485     -224702.87      0             -224642.36      97571.831    \n",
            "       440   249.76378     -224696.79      0             -224641.04      97973.38     \n",
            "       450   227.49837     -224690.66      0             -224639.87      98394.637    \n",
            "       460   261.45057     -224697.01      0             -224638.65      97837.345    \n",
            "       470   273.74621     -224698.42      0             -224637.31      97475.674    \n",
            "       480   231.68584     -224687.83      0             -224636.11      98076.544    \n",
            "       490   234.81567     -224687.41      0             -224634.99      98290.113    \n",
            "       500   296.79103     -224699.93      0             -224633.68      97696.052    \n",
            "       510   294.54775     -224698.07      0             -224632.32      97868.919    \n",
            "       520   245.08164     -224685.91      0             -224631.2       98621.49     \n",
            "       530   259.74919     -224688.11      0             -224630.13      98417.623    \n",
            "       540   300.59123     -224696         0             -224628.9       97792.779    \n",
            "       550   283.41157     -224690.97      0             -224627.71      97969.714    \n",
            "       560   258.02108     -224684.26      0             -224626.66      98108.543    \n",
            "       570   284.05556     -224689         0             -224625.59      97240.575    \n",
            "       580   285.65381     -224688.22      0             -224624.45      96684.354    \n",
            "       590   241.04887     -224677.24      0             -224623.43      97185.134    \n",
            "       600   249.07423     -224678.02      0             -224622.41      97468.475    \n",
            "       610   306.48392     -224689.62      0             -224621.2       97340.096    \n",
            "       620   312.44104     -224689.71      0             -224619.96      97981.418    \n",
            "       630   285.1831      -224682.58      0             -224618.92      99001.576    \n",
            "       640   312.73027     -224687.75      0             -224617.94      99027.514    \n",
            "       650   344.32172     -224693.84      0             -224616.98      98651.786    \n",
            "       660   313.15674     -224686.12      0             -224616.21      98880.942    \n",
            "       670   292.34622     -224680.88      0             -224615.62      98798.338    \n",
            "       680   314.39871     -224685.21      0             -224615.03      97955.218    \n",
            "       690   309.67242     -224683.6       0             -224614.47      97558.356    \n",
            "       700   276.69317     -224675.74      0             -224613.97      97887.838    \n",
            "       710   289.84125     -224678.15      0             -224613.45      97765.658    \n",
            "       720   322.91917     -224684.94      0             -224612.86      97273.636    \n",
            "       730   302.60418     -224679.87      0             -224612.32      97436.589    \n",
            "       740   279.01768     -224674.13      0             -224611.84      97708.886    \n",
            "       750   309.60164     -224680.44      0             -224611.33      97268.6      \n",
            "       760   330.08467     -224684.48      0             -224610.8       97003.734    \n",
            "       770   295.83699     -224676.42      0             -224610.38      97679.771    \n",
            "       780   281.75042     -224672.91      0             -224610.01      98301.841    \n",
            "       790   326.94408     -224682.57      0             -224609.58      98224.35     \n",
            "       800   352.00965     -224687.8       0             -224609.22      98367.114    \n",
            "       810   324.07719     -224681.42      0             -224609.07      99001.857    \n",
            "       820   313.96885     -224679.16      0             -224609.07      99039.556    \n",
            "       830   337.38973     -224684.49      0             -224609.17      98334.655    \n",
            "       840   332.14072     -224683.58      0             -224609.43      97945.97     \n",
            "       850   300.95345     -224676.99      0             -224609.81      97947.073    \n",
            "       860   297.74543     -224676.65      0             -224610.19      97601.804    \n",
            "       870   313.22881     -224680.5       0             -224610.57      97172.51     \n",
            "       880   305.55741     -224679.22      0             -224611.01      97314.987    \n",
            "       890   288.20551     -224675.77      0             -224611.43      97725.514    \n",
            "       900   296.72647     -224678.05      0             -224611.81      97872.354    \n",
            "       910   316.435       -224682.84      0             -224612.2       97989.581    \n",
            "       920   319.27453     -224683.94      0             -224612.66      98325.471    \n",
            "       930   314.39349     -224683.39      0             -224613.21      98576.647    \n",
            "       940   317.00794     -224684.57      0             -224613.81      98615.765    \n",
            "       950   321.15365     -224686.18      0             -224614.49      98622.375    \n",
            "       960   315.84121     -224685.74      0             -224615.24      98541.263    \n",
            "       970   301.03967     -224683.21      0             -224616         98203.634    \n",
            "       980   287.45317     -224680.88      0             -224616.71      97828.731    \n",
            "       990   288.54562     -224681.74      0             -224617.33      97666.25     \n",
            "      1000   298.20757     -224684.47      0             -224617.9       97663.871    \n",
            "      1010   297.14934     -224684.79      0             -224618.45      97798.49     \n",
            "      1020   291.98265     -224684.15      0             -224618.97      98086.199    \n",
            "      1030   292.36175     -224684.71      0             -224619.45      98440.88     \n",
            "      1040   299.95908     -224686.85      0             -224619.89      98609.904    \n",
            "      1050   308.83629     -224689.29      0             -224620.34      98499.746    \n",
            "      1060   302.85137     -224688.43      0             -224620.83      98311.052    \n",
            "      1070   286.08736     -224685.15      0             -224621.29      98011.198    \n",
            "      1080   276.28496     -224683.35      0             -224621.67      97536.201    \n",
            "      1090   276.13559     -224683.59      0             -224621.95      97177.861    \n",
            "      1100   278.59497     -224684.32      0             -224622.13      97183.383    \n",
            "      1110   277.36169     -224684.14      0             -224622.22      97478.772    \n",
            "      1120   278.84886     -224684.47      0             -224622.22      97840.556    \n",
            "      1130   289.68568     -224686.8       0             -224622.13      98192.199    \n",
            "      1140   298.46659     -224688.62      0             -224622         98500.843    \n",
            "      1150   298.62981     -224688.51      0             -224621.85      98629.45     \n",
            "      1160   297.69819     -224688.15      0             -224621.69      98529.547    \n",
            "      1170   295.94897     -224687.59      0             -224621.53      98369.415    \n",
            "      1180   290.73061     -224686.24      0             -224621.34      98194.435    \n",
            "      1190   289.32732     -224685.71      0             -224621.12      97887.013    \n",
            "      1200   288.83261     -224685.34      0             -224620.86      97649.018    \n",
            "      1210   285.2652      -224684.23      0             -224620.55      97643.597    \n",
            "      1220   286.65397     -224684.17      0             -224620.18      97682.291    \n",
            "      1230   294.28925     -224685.46      0             -224619.76      97698.732    \n",
            "      1240   297.06535     -224685.62      0             -224619.31      97854.878    \n",
            "      1250   289.20274     -224683.4       0             -224618.84      98153.468    \n",
            "      1260   292.13901     -224683.57      0             -224618.36      98206.356    \n",
            "      1270   303.78525     -224685.65      0             -224617.84      98034.511    \n",
            "      1280   298.70034     -224684         0             -224617.32      98021.398    \n",
            "      1290   289.36729     -224681.42      0             -224616.82      98069.025    \n",
            "      1300   295.27883     -224682.22      0             -224616.31      98023.373    \n",
            "      1310   307.57026     -224684.42      0             -224615.76      98055.228    \n",
            "      1320   309.80736     -224684.4       0             -224615.24      98345.609    \n",
            "      1330   310.34151     -224684.05      0             -224614.77      98651.483    \n",
            "      1340   316.15616     -224684.94      0             -224614.36      98711.509    \n",
            "      1350   317.21224     -224684.83      0             -224614.02      98570.695    \n",
            "      1360   311.46049     -224683.28      0             -224613.76      98273.517    \n",
            "      1370   304.11267     -224681.44      0             -224613.55      97851.602    \n",
            "      1380   299.21548     -224680.17      0             -224613.38      97434.825    \n",
            "      1390   288.35377     -224677.56      0             -224613.19      97295.519    \n",
            "      1400   288.66664     -224677.41      0             -224612.97      97292.293    \n",
            "      1410   303.45068     -224680.45      0             -224612.71      97377.199    \n",
            "      1420   306.37733     -224680.83      0             -224612.43      97816.02     \n",
            "      1430   307.77297     -224680.9       0             -224612.19      98296.796    \n",
            "      1440   321.65156     -224683.8       0             -224611.99      98452.855    \n",
            "      1450   327.01693     -224684.88      0             -224611.88      98509.948    \n",
            "      1460   313.09307     -224681.78      0             -224611.89      98694.166    \n",
            "      1470   307.69076     -224680.65      0             -224611.96      98749.368    \n",
            "      1480   322.56393     -224684.1       0             -224612.09      98562.567    \n",
            "      1490   323.71945     -224684.58      0             -224612.32      98520.576    \n",
            "      1500   313.19108     -224682.56      0             -224612.65      98495.427    \n",
            "      1510   311.86285     -224682.65      0             -224613.04      98112.026    \n",
            "      1520   309.91431     -224682.66      0             -224613.48      97577.15     \n",
            "      1530   288.89862     -224678.42      0             -224613.93      97388.227    \n",
            "      1540   272.80462     -224675.19      0             -224614.29      97442.317    \n",
            "      1550   284.77503     -224678.13      0             -224614.56      97492.325    \n",
            "      1560   305.72265     -224683.02      0             -224614.78      97766.825    \n",
            "      1570   317.00769     -224685.79      0             -224615.03      98331.496    \n",
            "      1580   318.06773     -224686.36      0             -224615.36      98842.963    \n",
            "      1590   319.28776     -224687.05      0             -224615.77      98917.038    \n",
            "      1600   314.36949     -224686.45      0             -224616.27      98666.387    \n",
            "      1610   300.41814     -224683.86      0             -224616.8       98327.641    \n",
            "      1620   293.27774     -224682.78      0             -224617.31      97884.15     \n",
            "      1630   288.1994      -224682.1       0             -224617.77      97556.561    \n",
            "      1640   285.28957     -224681.85      0             -224618.16      97556.707    \n",
            "      1650   291.1588      -224683.49      0             -224618.5       97842.773    \n",
            "      1660   300.79668     -224685.95      0             -224618.8       98199.859    \n",
            "      1670   304.71593     -224687.13      0             -224619.11      98458.191    \n",
            "      1680   303.53391     -224687.2       0             -224619.44      98474.301    \n",
            "      1690   302.03716     -224687.21      0             -224619.78      98133.713    \n",
            "      1700   288.17364     -224684.45      0             -224620.12      97743.794    \n",
            "      1710   270.09155     -224680.67      0             -224620.37      97549.296    \n",
            "      1720   270.54316     -224680.91      0             -224620.52      97412.135    \n",
            "      1730   282.74673     -224683.66      0             -224620.54      97376.079    \n",
            "      1740   288.53617     -224684.91      0             -224620.49      97683.207    \n",
            "      1750   293.02169     -224685.8       0             -224620.39      98213.825    \n",
            "      1760   307.60651     -224688.93      0             -224620.27      98520.263    \n",
            "      1770   314.67144     -224690.41      0             -224620.16      98499.675    \n",
            "      1780   299.17583     -224686.9       0             -224620.12      98364.064    \n",
            "      1790   279.51266     -224682.46      0             -224620.07      98146.596    \n",
            "      1800   276.80457     -224681.74      0             -224619.94      97806.569    \n",
            "      1810   285.83926     -224683.54      0             -224619.73      97509.644    \n",
            "      1820   289.07226     -224683.97      0             -224619.44      97556.253    \n",
            "      1830   285.51123     -224682.84      0             -224619.11      97934.436    \n",
            "      1840   293.31325     -224684.2       0             -224618.72      98318.782    \n",
            "      1850   313.67675     -224688.32      0             -224618.29      98522.658    \n",
            "      1860   323.39646     -224690.1       0             -224617.91      98618.777    \n",
            "      1870   312.16526     -224687.31      0             -224617.63      98655.941    \n",
            "      1880   297.31411     -224683.79      0             -224617.42      98544.537    \n",
            "      1890   292.91014     -224682.59      0             -224617.2       98313.684    \n",
            "      1900   296.30702     -224683.11      0             -224616.97      98086.514    \n",
            "      1910   297.32832     -224683.08      0             -224616.71      97975.627    \n",
            "      1920   300.55662     -224683.53      0             -224616.44      97930.276    \n",
            "      1930   308.89783     -224685.12      0             -224616.17      97900.762    \n",
            "      1940   308.76646     -224684.86      0             -224615.93      97962.898    \n",
            "      1950   301.41224     -224683.02      0             -224615.74      98076.548    \n",
            "      1960   298.42165     -224682.17      0             -224615.55      98146.13     \n",
            "      1970   302.26515     -224682.84      0             -224615.37      98160.144    \n",
            "      1980   302.20694     -224682.65      0             -224615.18      98219.359    \n",
            "      1990   302.57854     -224682.56      0             -224615.01      98280.452    \n",
            "      2000   308.62677     -224683.75      0             -224614.85      98307.46     \n",
            "      2010   311.46338     -224684.26      0             -224614.73      98258.7      \n",
            "      2020   305.59493     -224682.88      0             -224614.66      98040.553    \n",
            "      2030   299.54681     -224681.48      0             -224614.61      97700.46     \n",
            "      2040   297.37026     -224680.95      0             -224614.56      97456.808    \n",
            "      2050   293.93781     -224680.12      0             -224614.5       97413.616    \n",
            "      2060   295.17305     -224680.31      0             -224614.42      97441.494    \n",
            "      2070   297.85793     -224680.81      0             -224614.32      97592.288    \n",
            "      2080   300.72342     -224681.33      0             -224614.2       97857.672    \n",
            "      2090   305.17331     -224682.21      0             -224614.09      98092.467    \n",
            "      2100   306.25467     -224682.36      0             -224613.99      98299.173    \n",
            "      2110   305.28337     -224682.08      0             -224613.93      98469.443    \n",
            "      2120   309.63214     -224683.01      0             -224613.89      98489.544    \n",
            "      2130   318.92545     -224685.09      0             -224613.9       98323.813    \n",
            "      2140   315.47344     -224684.41      0             -224613.99      98203.097    \n",
            "      2150   301.098       -224681.36      0             -224614.14      98186.542    \n",
            "      2160   298.85801     -224681.02      0             -224614.3       98048.421    \n",
            "      2170   306.90179     -224682.98      0             -224614.47      97867.652    \n",
            "      2180   306.28734     -224683.03      0             -224614.66      97904.035    \n",
            "      2190   300.86764     -224682.03      0             -224614.87      98044.239    \n",
            "      2200   304.85738     -224683.15      0             -224615.1       98095.362    \n",
            "      2210   308.57763     -224684.23      0             -224615.35      98188.293    \n",
            "      2220   300.84526     -224682.78      0             -224615.62      98381.174    \n",
            "      2230   298.66762     -224682.57      0             -224615.9       98396.028    \n",
            "      2240   307.08216     -224684.74      0             -224616.18      98143.381    \n",
            "      2250   305.92016     -224684.79      0             -224616.5       97876.339    \n",
            "      2260   290.04794     -224681.56      0             -224616.81      97767.67     \n",
            "      2270   284.66332     -224680.62      0             -224617.08      97715.602    \n",
            "      2280   296.20252     -224683.41      0             -224617.29      97698.561    \n",
            "      2290   301.60109     -224684.81      0             -224617.49      97890.078    \n",
            "      2300   299.93298     -224684.64      0             -224617.69      98181.558    \n",
            "      2310   300.95915     -224685.07      0             -224617.89      98361.93     \n",
            "      2320   304.24429     -224686.01      0             -224618.1       98375.469    \n",
            "      2330   305.35191     -224686.49      0             -224618.33      98245.842    \n",
            "      2340   296.51829     -224684.76      0             -224618.57      98083.377    \n",
            "      2350   286.07019     -224682.65      0             -224618.79      97932.04     \n",
            "      2360   287.17887     -224683.06      0             -224618.95      97846.034    \n",
            "      2370   291.4441      -224684.12      0             -224619.06      97920.167    \n",
            "      2380   294.02088     -224684.77      0             -224619.13      98111.904    \n",
            "      2390   296.23058     -224685.31      0             -224619.18      98365.831    \n",
            "      2400   304.86478     -224687.27      0             -224619.21      98532.335    \n",
            "      2410   313.22035     -224689.19      0             -224619.27      98541.478    \n",
            "      2420   304.75704     -224687.41      0             -224619.38      98461.671    \n",
            "      2430   291.6857      -224684.62      0             -224619.51      98203.338    \n",
            "      2440   288.75066     -224684.06      0             -224619.6       97776.972    \n",
            "      2450   286.1122      -224683.51      0             -224619.64      97469.267    \n",
            "      2460   278.96065     -224681.9       0             -224619.62      97453.468    \n",
            "      2470   280.67861     -224682.18      0             -224619.52      97609.399    \n",
            "      2480   291.12289     -224684.33      0             -224619.34      97868.679    \n",
            "      2490   300.66928     -224686.23      0             -224619.11      98190.246    \n",
            "      2500   304.76074     -224686.91      0             -224618.88      98446.653    \n",
            "      2510   308.52997     -224687.54      0             -224618.66      98497.87     \n",
            "      2520   306.14997     -224686.83      0             -224618.49      98395.223    \n",
            "      2530   298.98606     -224685.08      0             -224618.34      98172.511    \n",
            "      2540   290.00218     -224682.92      0             -224618.18      97897.409    \n",
            "      2550   284.54607     -224681.52      0             -224618         97632.377    \n",
            "      2560   286.3173      -224681.66      0             -224617.75      97410.35     \n",
            "      2570   287.28357     -224681.57      0             -224617.44      97438.427    \n",
            "      2580   291.7156      -224682.2       0             -224617.08      97741.106    \n",
            "      2590   302.80221     -224684.27      0             -224616.68      98128.082    \n",
            "      2600   309.65499     -224685.41      0             -224616.28      98497.241    \n",
            "      2610   312.38276     -224685.66      0             -224615.93      98712.602    \n",
            "      2620   312.65642     -224685.43      0             -224615.64      98744.433    \n",
            "      2630   311.48858     -224684.94      0             -224615.4       98550.283    \n",
            "      2640   308.39598     -224684.07      0             -224615.22      98168.752    \n",
            "      2650   297.40974     -224681.47      0             -224615.08      97834.325    \n",
            "      2660   289.3005      -224679.52      0             -224614.93      97603.223    \n",
            "      2670   293.7409      -224680.32      0             -224614.75      97484.375    \n",
            "      2680   302.56096     -224682.07      0             -224614.53      97566.725    \n",
            "      2690   309.16446     -224683.34      0             -224614.32      97818.918    \n",
            "      2700   306.92545     -224682.66      0             -224614.14      98173.011    \n",
            "      2710   304.27267     -224681.92      0             -224614         98373.535    \n",
            "      2720   309.46961     -224682.97      0             -224613.88      98237.622    \n",
            "      2730   311.3921      -224683.32      0             -224613.8       97942.854    \n",
            "      2740   298.70333     -224680.45      0             -224613.77      97756.028    \n",
            "      2750   287.21127     -224677.84      0             -224613.73      97650.281    \n",
            "      2760   295.81119     -224679.68      0             -224613.65      97578.402    \n",
            "      2770   307.55746     -224682.2       0             -224613.54      97790.164    \n",
            "      2780   312.07528     -224683.13      0             -224613.47      98251.25     \n",
            "      2790   314.22365     -224683.59      0             -224613.44      98684.758    \n",
            "      2800   322.61807     -224685.51      0             -224613.49      98839.967    \n",
            "      2810   325.50203     -224686.31      0             -224613.65      98841.073    \n",
            "      2820   317.20646     -224684.73      0             -224613.92      98828.803    \n",
            "      2830   309.52627     -224683.36      0             -224614.26      98690.067    \n",
            "      2840   303.13657     -224682.31      0             -224614.64      98450.994    \n",
            "      2850   304.17569     -224682.92      0             -224615.02      98133.422    \n",
            "      2860   304.22555     -224683.33      0             -224615.42      97886.11     \n",
            "      2870   297.29152     -224682.19      0             -224615.82      97789.519    \n",
            "      2880   290.28094     -224680.99      0             -224616.19      97765.386    \n",
            "      2890   291.59244     -224681.61      0             -224616.52      97751.938    \n",
            "      2900   295.56944     -224682.79      0             -224616.81      97806.691    \n",
            "      2910   292.48243     -224682.36      0             -224617.07      97991.804    \n",
            "      2920   293.52685     -224682.83      0             -224617.3       98170.022    \n",
            "      2930   301.2476      -224684.76      0             -224617.51      98278.599    \n",
            "      2940   309.07879     -224686.73      0             -224617.73      98299.254    \n",
            "      2950   306.16326     -224686.33      0             -224617.99      98255.418    \n",
            "      2960   294.49376     -224684         0             -224618.26      98190.843    \n",
            "      2970   291.18394     -224683.5       0             -224618.5       98043.383    \n",
            "      2980   291.49635     -224683.78      0             -224618.71      97936.159    \n",
            "      2990   288.85023     -224683.35      0             -224618.87      97975.768    \n",
            "      3000   291.45736     -224684.06      0             -224618.99      98091.314    \n",
            "      3010   300.72896     -224686.21      0             -224619.08      98203.157    \n",
            "      3020   308.32809     -224688.01      0             -224619.18      98263.874    \n",
            "      3030   299.42102     -224686.14      0             -224619.3       98362.75     \n",
            "      3040   289.62036     -224684.07      0             -224619.42      98359.525    \n",
            "      3050   290.90936     -224684.43      0             -224619.49      98154.622    \n",
            "      3060   291.82238     -224684.68      0             -224619.53      97883.475    \n",
            "      3070   287.6375      -224683.74      0             -224619.53      97708.822    \n",
            "      3080   283.87675     -224682.85      0             -224619.48      97681.194    \n",
            "      3090   288.64636     -224683.8       0             -224619.36      97712.835    \n",
            "      3100   294.21042     -224684.87      0             -224619.19      97830.645    \n",
            "      3110   292.49735     -224684.28      0             -224618.99      98073.753    \n",
            "      3120   293.29332     -224684.23      0             -224618.76      98309.991    \n",
            "      3130   302.26286     -224685.98      0             -224618.5       98444.878    \n",
            "      3140   307.48385     -224686.88      0             -224618.24      98541.478    \n",
            "      3150   306.6487      -224686.47      0             -224618.02      98617.002    \n",
            "      3160   305.61281     -224686.05      0             -224617.82      98640.798    \n",
            "      3170   305.9681      -224685.96      0             -224617.66      98570.719    \n",
            "      3180   304.56596     -224685.5       0             -224617.52      98370.769    \n",
            "      3190   297.811       -224683.88      0             -224617.39      98104.665    \n",
            "      3200   288.18209     -224681.6       0             -224617.26      97842.686    \n",
            "      3210   283.30713     -224680.33      0             -224617.09      97596.496    \n",
            "      3220   284.29204     -224680.31      0             -224616.85      97477.459    \n",
            "      3230   289.29354     -224681.12      0             -224616.54      97624.114    \n",
            "      3240   300.19787     -224683.2       0             -224616.19      97966.206    \n",
            "      3250   307.64284     -224684.5       0             -224615.82      98422.62     \n",
            "      3260   312.3995      -224685.23      0             -224615.49      98827.146    \n",
            "      3270   316.96577     -224685.98      0             -224615.22      98976.594    \n",
            "      3280   318.9332      -224686.22      0             -224615.03      98794.033    \n",
            "      3290   311.76084     -224684.51      0             -224614.92      98419.649    \n",
            "      3300   295.73027     -224680.88      0             -224614.86      98065.712    \n",
            "      3310   286.63356     -224678.78      0             -224614.79      97844.307    \n",
            "      3320   292.25388     -224679.92      0             -224614.68      97794.464    \n",
            "      3330   300.81262     -224681.67      0             -224614.52      97923.237    \n",
            "      3340   302.81497     -224681.96      0             -224614.36      98177.774    \n",
            "      3350   311.29687     -224683.71      0             -224614.22      98334.795    \n",
            "      3360   321.26313     -224685.84      0             -224614.12      98373.079    \n",
            "      3370   312.59834     -224683.9       0             -224614.11      98412.132    \n",
            "      3380   297.95064     -224680.67      0             -224614.16      98352.22     \n",
            "      3390   295.71546     -224680.22      0             -224614.21      98170.852    \n",
            "      3400   305.26496     -224682.39      0             -224614.24      98042.533    \n",
            "      3410   308.33363     -224683.12      0             -224614.29      98214.013    \n",
            "      3420   305.41559     -224682.56      0             -224614.38      98554.321    \n",
            "      3430   315.71461     -224684.98      0             -224614.5       98633.717    \n",
            "      3440   322.26137     -224686.65      0             -224614.71      98500.699    \n",
            "      3450   309.30907     -224684.05      0             -224615         98333.165    \n",
            "      3460   291.42445     -224680.37      0             -224615.31      98109.845    \n",
            "      3470   286.94871     -224679.64      0             -224615.58      97808.417    \n",
            "      3480   294.04171     -224681.44      0             -224615.8       97617.735    \n",
            "      3490   294.61402     -224681.76      0             -224615.99      97773.467    \n",
            "      3500   296.14379     -224682.26      0             -224616.15      98086.966    \n",
            "      3510   306.90227     -224684.82      0             -224616.31      98343.458    \n",
            "      3520   316.70768     -224687.2       0             -224616.51      98501.785    \n",
            "      3530   316.94917     -224687.53      0             -224616.78      98554.806    \n",
            "      3540   300.65316     -224684.23      0             -224617.11      98584.969    \n",
            "      3550   290.7781      -224682.34      0             -224617.43      98401.978    \n",
            "      3560   297.60953     -224684.15      0             -224617.71      97999.044    \n",
            "      3570   294.94115     -224683.82      0             -224617.98      97783.865    \n",
            "      3580   284.14214     -224681.64      0             -224618.21      97827.54     \n",
            "      3590   286.43489     -224682.31      0             -224618.37      97877.315    \n",
            "      3600   297.77135     -224684.96      0             -224618.49      97826.6      \n",
            "      3610   298.35079     -224685.19      0             -224618.59      97796.903    \n",
            "      3620   290.27999     -224683.47      0             -224618.67      97838.365    \n",
            "      3630   287.54395     -224682.91      0             -224618.72      97887.094    \n",
            "      3640   294.42448     -224684.45      0             -224618.72      97919.256    \n",
            "      3650   296.96475     -224684.99      0             -224618.7       98023.772    \n",
            "      3660   286.80122     -224682.67      0             -224618.65      98270.113    \n",
            "      3670   291.32148     -224683.58      0             -224618.55      98404.835    \n",
            "      3680   304.71208     -224686.44      0             -224618.42      98464.769    \n",
            "      3690   308.36497     -224687.13      0             -224618.29      98533.705    \n",
            "      3700   303.81579     -224686.03      0             -224618.21      98479.97     \n",
            "      3710   292.0339      -224683.32      0             -224618.13      98365.04     \n",
            "      3720   292.58855     -224683.35      0             -224618.03      98162.398    \n",
            "      3730   301.41589     -224685.19      0             -224617.9       98054.978    \n",
            "      3740   301.03991     -224684.97      0             -224617.77      98203.564    \n",
            "      3750   297.44311     -224684.03      0             -224617.63      98369.59     \n",
            "      3760   297.7948      -224683.97      0             -224617.49      98359.67     \n",
            "      3770   300.89325     -224684.52      0             -224617.35      98219.582    \n",
            "      3780   300.91695     -224684.37      0             -224617.2       98101.939    \n",
            "      3790   298.39332     -224683.66      0             -224617.05      98035.56     \n",
            "      3800   297.50292     -224683.31      0             -224616.9       98031.177    \n",
            "      3810   299.18062     -224683.52      0             -224616.74      98144.076    \n",
            "      3820   306.21792     -224684.93      0             -224616.57      98360.388    \n",
            "      3830   311.04652     -224685.86      0             -224616.43      98706.078    \n",
            "      3840   312.53567     -224686.1       0             -224616.33      98995.042    \n",
            "      3850   313.43001     -224686.26      0             -224616.3       98993.261    \n",
            "      3860   313.92307     -224686.4       0             -224616.32      98648.854    \n",
            "      3870   308.34124     -224685.24      0             -224616.41      98112.068    \n",
            "      3880   287.95101     -224680.8       0             -224616.52      97697.538    \n",
            "      3890   277.01278     -224678.42      0             -224616.59      97334.114    \n",
            "      3900   283.88192     -224679.94      0             -224616.56      97033.094    \n",
            "      3910   284.565       -224679.99      0             -224616.47      97072.582    \n",
            "      3920   278.70272     -224678.51      0             -224616.3       97427.018    \n",
            "      3930   285.52904     -224679.79      0             -224616.05      97833.462    \n",
            "      3940   307.30881     -224684.33      0             -224615.73      98191.934    \n",
            "      3950   318.83124     -224686.6       0             -224615.43      98645.246    \n",
            "      3960   317.94648     -224686.18      0             -224615.21      99056.644    \n",
            "      3970   318.42642     -224686.16      0             -224615.08      99204.937    \n",
            "      3980   318.8448      -224686.21      0             -224615.03      99044.722    \n",
            "      3990   314.37666     -224685.25      0             -224615.07      98623.424    \n",
            "      4000   301.59082     -224682.5       0             -224615.18      98165.411    \n",
            "      4010   295.44406     -224681.24      0             -224615.29      97714.263    \n",
            "      4020   292.42019     -224680.66      0             -224615.38      97400.518    \n",
            "      4030   288.82321     -224679.92      0             -224615.44      97303.364    \n",
            "      4040   289.50155     -224680.08      0             -224615.45      97439.686    \n",
            "      4050   294.00336     -224681.05      0             -224615.42      97827.9      \n",
            "      4060   305.19143     -224683.5       0             -224615.37      98296.205    \n",
            "      4070   314.36783     -224685.51      0             -224615.33      98720.091    \n",
            "      4080   319.71796     -224686.73      0             -224615.36      98892.054    \n",
            "      4090   313.76373     -224685.52      0             -224615.48      98843.773    \n",
            "      4100   302.98437     -224683.29      0             -224615.65      98620.47     \n",
            "      4110   302.30217     -224683.33      0             -224615.85      98219.669    \n",
            "      4120   296.76059     -224682.3       0             -224616.05      97925.893    \n",
            "      4130   291.50484     -224681.3       0             -224616.23      97709.205    \n",
            "      4140   291.50428     -224681.45      0             -224616.38      97566.468    \n",
            "      4150   293.75748     -224682.06      0             -224616.49      97591.804    \n",
            "      4160   298.05466     -224683.11      0             -224616.58      97777.223    \n",
            "      4170   299.6098      -224683.53      0             -224616.65      98073.79     \n",
            "      4180   303.92174     -224684.58      0             -224616.73      98300.088    \n",
            "      4190   303.06554     -224684.48      0             -224616.83      98493.667    \n",
            "      4200   300.30777     -224683.97      0             -224616.94      98587.834    \n",
            "      4210   304.47833     -224685.02      0             -224617.05      98491.728    \n",
            "      4220   304.66185     -224685.2       0             -224617.19      98403.237    \n",
            "      4230   298.23361     -224683.92      0             -224617.34      98371.776    \n",
            "      4240   299.60921     -224684.38      0             -224617.5       98220.423    \n",
            "      4250   303.62129     -224685.43      0             -224617.65      98034.417    \n",
            "      4260   297.42566     -224684.21      0             -224617.81      98055.44     \n",
            "      4270   290.95133     -224682.91      0             -224617.96      98135.895    \n",
            "      4280   294.72297     -224683.87      0             -224618.08      98120.211    \n",
            "      4290   300.39367     -224685.23      0             -224618.18      98125.924    \n",
            "      4300   297.00843     -224684.57      0             -224618.27      98231.93     \n",
            "      4310   290.41746     -224683.18      0             -224618.35      98300.302    \n",
            "      4320   296.3238      -224684.55      0             -224618.4       98173.389    \n",
            "      4330   298.83905     -224685.14      0             -224618.42      98134.231    \n",
            "      4340   295.58728     -224684.43      0             -224618.44      98227.563    \n",
            "      4350   296.55678     -224684.65      0             -224618.45      98307.148    \n",
            "      4360   302.96378     -224686.07      0             -224618.44      98328.267    \n",
            "      4370   308.72313     -224687.36      0             -224618.44      98322.343    \n",
            "      4380   300.30909     -224685.52      0             -224618.48      98371.411    \n",
            "      4390   294.28727     -224684.21      0             -224618.52      98238.127    \n",
            "      4400   292.09951     -224683.74      0             -224618.54      97967.513    \n",
            "      4410   284.61072     -224682.05      0             -224618.51      97770.687    \n",
            "      4420   283.34638     -224681.68      0             -224618.43      97590.131    \n",
            "      4430   287.34098     -224682.42      0             -224618.28      97553.778    \n",
            "      4440   293.72649     -224683.63      0             -224618.06      97782.507    \n",
            "      4450   300.51665     -224684.9       0             -224617.81      98172.414    \n",
            "      4460   305.08695     -224685.66      0             -224617.56      98540.912    \n",
            "      4470   309.91118     -224686.51      0             -224617.32      98752.621    \n",
            "      4480   310.96892     -224686.55      0             -224617.14      98747.669    \n",
            "      4490   306.88039     -224685.5       0             -224617         98495.231    \n",
            "      4500   297.57041     -224683.32      0             -224616.89      98148.188    \n",
            "      4510   291.08621     -224681.76      0             -224616.78      97831.176    \n",
            "      4520   291.50552     -224681.7       0             -224616.63      97567.017    \n",
            "      4530   291.78209     -224681.57      0             -224616.44      97420.169    \n",
            "      4540   290.70129     -224681.11      0             -224616.21      97477.182    \n",
            "      4550   294.06678     -224681.59      0             -224615.95      97702.173    \n",
            "      4560   298.9186      -224682.38      0             -224615.66      97993.32     \n",
            "      4570   305.99916     -224683.67      0             -224615.36      98173.331    \n",
            "      4580   310.22356     -224684.35      0             -224615.1       98242.05     \n",
            "      4590   306.81728     -224683.37      0             -224614.88      98305.289    \n",
            "      4600   305.71486     -224682.95      0             -224614.71      98302.347    \n",
            "      4610   306.05823     -224682.88      0             -224614.56      98297.249    \n",
            "      4620   308.17926     -224683.24      0             -224614.45      98330.734    \n",
            "      4630   309.61004     -224683.49      0             -224614.37      98323.256    \n",
            "      4640   304.94022     -224682.41      0             -224614.34      98241.806    \n",
            "      4650   303.01655     -224681.97      0             -224614.33      98120.209    \n",
            "      4660   303.42506     -224682.06      0             -224614.33      98088.217    \n",
            "      4670   305.82316     -224682.62      0             -224614.35      98133.624    \n",
            "      4680   307.84348     -224683.12      0             -224614.4       98232.094    \n",
            "      4690   305.96512     -224682.78      0             -224614.48      98365.318    \n",
            "      4700   307.62341     -224683.27      0             -224614.6       98425.439    \n",
            "      4710   310.57162     -224684.09      0             -224614.76      98422.645    \n",
            "      4720   310.14919     -224684.21      0             -224614.98      98414.556    \n",
            "      4730   306.17669     -224683.59      0             -224615.25      98378.912    \n",
            "      4740   303.40133     -224683.28      0             -224615.55      98201.425    \n",
            "      4750   302.92574     -224683.49      0             -224615.87      97902.325    \n",
            "      4760   292.87536     -224681.57      0             -224616.19      97776.081    \n",
            "      4770   284.87309     -224680.05      0             -224616.46      97819.313    \n",
            "      4780   293.83836     -224682.26      0             -224616.67      97825.441    \n",
            "      4790   303.83402     -224684.67      0             -224616.85      97888.58     \n",
            "      4800   302.09384     -224684.47      0             -224617.03      98099.812    \n",
            "      4810   295.58479     -224683.19      0             -224617.21      98310.704    \n",
            "      4820   298.74498     -224684.06      0             -224617.37      98350.206    \n",
            "      4830   306.67293     -224685.98      0             -224617.52      98316.24     \n",
            "      4840   307.55731     -224686.35      0             -224617.69      98405.023    \n",
            "      4850   300.37119     -224684.94      0             -224617.89      98531.75     \n",
            "      4860   300.20521     -224685.09      0             -224618.08      98408.119    \n",
            "      4870   302.33245     -224685.76      0             -224618.27      98156.609    \n",
            "      4880   293.63218     -224684         0             -224618.45      98006.468    \n",
            "      4890   283.76715     -224681.93      0             -224618.58      97839.1      \n",
            "      4900   283.46085     -224681.93      0             -224618.65      97615.307    \n",
            "      4910   283.98412     -224682.04      0             -224618.65      97575.578    \n",
            "      4920   288.59647     -224683.01      0             -224618.58      97720.095    \n",
            "      4930   295.76553     -224684.49      0             -224618.47      97954.404    \n",
            "      4940   300.76261     -224685.47      0             -224618.33      98209.868    \n",
            "      4950   303.53684     -224685.96      0             -224618.2       98368.022    \n",
            "      4960   300.00306     -224685.06      0             -224618.09      98374.14     \n",
            "      4970   295.55188     -224683.96      0             -224617.98      98266.01     \n",
            "      4980   293.93894     -224683.47      0             -224617.86      98219.405    \n",
            "      4990   295.36347     -224683.64      0             -224617.71      98321.413    \n",
            "      5000   304.20187     -224685.45      0             -224617.55      98460.866    \n",
            "Loop time of 163.471 on 1 procs for 5000 steps with 1728 atoms\n",
            "\n",
            "Performance: 2.643 ns/day, 9.082 hours/ns, 30.586 timesteps/s, 52.853 katom-step/s\n",
            "84.9% CPU use with 1 MPI tasks x 1 OpenMP threads\n",
            "\n",
            "MPI task timing breakdown:\n",
            "Section |  min time  |  avg time  |  max time  |%varavg| %total\n",
            "---------------------------------------------------------------\n",
            "Pair    | 159.87     | 159.87     | 159.87     |   0.0 | 97.80\n",
            "Neigh   | 0          | 0          | 0          |   0.0 |  0.00\n",
            "Comm    | 0.17214    | 0.17214    | 0.17214    |   0.0 |  0.11\n",
            "Output  | 0.13063    | 0.13063    | 0.13063    |   0.0 |  0.08\n",
            "Modify  | 3.2094     | 3.2094     | 3.2094     |   0.0 |  1.96\n",
            "Other   |            | 0.08581    |            |       |  0.05\n",
            "\n",
            "Nlocal:           1728 ave        1728 max        1728 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Nghost:           3645 ave        3645 max        3645 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "Neighs:          37592 ave       37592 max       37592 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "FullNghs:        79488 ave       79488 max       79488 min\n",
            "Histogram: 1 0 0 0 0 0 0 0 0 0\n",
            "\n",
            "Total # of neighbors = 79488\n",
            "Ave neighs/atom = 46\n",
            "Neighbor list builds = 0\n",
            "Dangerous builds = 0\n",
            "Total wall time: 0:02:50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!gdown --no-cookies 1aa2Kga_w-Zcw6BsmzJqH67NPcVwgHPS- --output Si_exp_1.txt\n",
        "!mv ./Si_exp_1.txt Si_run/\n",
        "\n",
        "with open(\"./Si_run/si.rdf\", \"r\") as f:\n",
        "    data_allegro = f.readlines()\n",
        "    data_allegro = np.array([x.split() for x in data_allegro[4:]]).astype(float)\n",
        "\n",
        "with open(\"./Si_run/Si_exp_1.txt\", \"r\") as f:\n",
        "    data_exp = f.readlines()\n",
        "    data_exp = np.array([x.split() for x in data_exp[3:]]).astype(float)\n",
        "\n",
        "plt.figure(figsize=(15,8))\n",
        "plt.plot(data_allegro[:,1], data_allegro[:,2], label=\"Si, Allegro, $T=300K$\")\n",
        "plt.plot(data_exp[:,0], data_exp[:,1], label=\"Si, some Exp\")\n",
        "\n",
        "plt.xlim(1.5, 4.0)\n",
        "plt.xlabel('r [$\\AA$]')\n",
        "plt.ylabel('g(r)')\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "bpzaaRXtEjLL",
        "outputId": "f62c78a0-b011-4b1d-dd1f-af07523a8e9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1aa2Kga_w-Zcw6BsmzJqH67NPcVwgHPS-\n",
            "To: /content/Si_exp_1.txt\n",
            "\r  0% 0.00/1.78k [00:00<?, ?B/s]\r100% 1.78k/1.78k [00:00<00:00, 9.07MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}